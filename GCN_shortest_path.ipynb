{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_shortest_path.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pesk5QkHvbYl"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSpSp-KeoKdy"
      },
      "source": [
        "<center><h1><font color=yellow size=40px>Tutorial about shortest path prediction with GCNs</font></h1></center>\n",
        "<br>\n",
        "<br>\n",
        "<center><img src=\"https://raw.githubusercontent.com/AlbertoZandara/GCN/main/Misc/alina-grubnyak-ZiQkhI7417A-unsplash.jpg\"/></center>\n",
        "\n",
        "In this notebook it is presented an original approach to predict the shortest path between to nodes in a graph using a Graph Convolutional Neural Network. The pipeline is based on the model introduced by Thomas Kipf in 2017, specifically adapted and implemented for shortest path prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v530zRq24BsO"
      },
      "source": [
        "## Scripts upload\n",
        "\n",
        "In first instance it is necessary to upload the libraries from the Kipf GCN Model in order to take advantage of that stucture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWYi9oezgfKr"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a936hWqhzTh"
      },
      "source": [
        "! mkdir /content/gcn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO-gCMeZh2lv"
      },
      "source": [
        "! mv __init__.py gcn\n",
        "! mv inits.py gcn\n",
        "! mv layers.py gcn\n",
        "! mv metrics.py gcn\n",
        "! mv models.py gcn\n",
        "! mv train.py gcn\n",
        "! mv utils.py gcn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BTUbeqEi3ps"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIzcakrWidD-"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from gcn.utils import *\n",
        "from gcn.inits import *\n",
        "from gcn.models import Model,MLP\n",
        "from gcn.layers import *\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAEixqwiiz6Q"
      },
      "source": [
        "## Graph initialization\n",
        "\n",
        "In this phase the dataset is initialized. It consists on 35000 graphs of 20 nodes not connected between each other. It is generated with Networkx as a \"partition graph\". A partition graph is a graph of subgraphs of defined size (20 in this case) where the nodes belonging to the same subgraph are connected with probability $P_{in}$ (0.3 in this case) and the different subgraphs are connected with probability $P_{out}$ (0 in this case). \n",
        "<br>\n",
        "<br>\n",
        "\n",
        "***WARNING: To perform this notebook at least 25GB of RAM memory are required!***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO4qDnGFjPpi"
      },
      "source": [
        "num_community = 35000                                     #Definition of the number of subgraphs in the graph\n",
        "node = [20 for i in range(num_community)]                 #Definition of the number of nodes for every subgraph\n",
        "GG = nx.random_partition_graph(node,.3,.0,seed=123)       #Definition of the graph with networkx, the probability of interconnection between partitions in .0 so all the partitions are isolated\n",
        "adj_GG = np.zeros((num_community, num_community))         #Definition of a matrix 35000x35000, the partitions are assumed to nodes\n",
        "for edge in GG.edges():\n",
        "    row = edge[0] // num_community                        #Belonging partition of the origin node\n",
        "    col = edge[1] // num_community                        #Belonging partition of the destination node\n",
        "    if row != col:                                        #If the partitions of appartenence are different put a 1 in the relative cell\n",
        "        adj_GG[row][col]=1                                \n",
        "        adj_GG[col][row]=1\n",
        "    \n",
        "adj_sparse = sparse.csr_matrix(adj_GG)                    #Adjaciency matrix of the communities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXxOoIhHj-sa"
      },
      "source": [
        "## Compiling features\n",
        "\n",
        "After the initializzation of the graph it is necessary to compile the features matrix. In a partition graphs each partition is considered as a single node (35000 in this case). For this reason it is necessary to fill in the features matrix of each partition (just like a node) the adjaciency matrix of the specific partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBScFEgUlXDz"
      },
      "source": [
        "partition = GG.graph['partition']                           #This is another graph with 35000 nodes in which subgraphs are assumed as nodes. Is a container of nodes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEPrvIatlYYB"
      },
      "source": [
        "adjlist = [[]]\n",
        "for i in range(len(partition)):                             #len(partition) is 35000\n",
        "    H = GG.subgraph(partition[i])                           #the i-th subgraph\n",
        "    adj = nx.adjacency_matrix(H).todense().tolist()         #Is a python list containing the adjaciency matrix 2D\n",
        "    for element in adj:\n",
        "        adjlist[i].extend(element)                          #Is a 2D matrix with the adjaciency matrix (reported as a list) of the i-th subgraph in the i-th raw\n",
        "    adjlist.append([])\n",
        "    \n",
        "adjlist = adjlist[:-1]\n",
        "adj_input = np.array(adjlist)\n",
        "features_sparse = sparse.csr_matrix(adj_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBeT0yfxljs8"
      },
      "source": [
        "path = []                                                    #Is a vector of 35000 components. It contains the shortest path between node 1 and 20 for each partition\n",
        "for i in range(num_community):\n",
        "    try:\n",
        "        path.append([nx.shortest_path(GG.subgraph(partition[i]), source=i*20, target=(i+1)*20-1)[1]-i*20])\n",
        "    except:\n",
        "        path.append([0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv3N2IUyv3w9"
      },
      "source": [
        "## Compiling labels\n",
        "\n",
        "Now it is necessary to associate the right output to each subgraph. \n",
        "\n",
        "The target is to obtain a network able to recognize the shortest path between 2 nodes. The problem lies in the fact that the output of the netwotk is normalized and has a defined size. \n",
        "The easier way to resolve this problem is using a network able to recognize the first node of the shortest path sequence and use this network iteratively until the output node correspond to the required destination.\n",
        "\n",
        "In this way the output has a precise dimension, a vector of 19 components (one for each node), and the following node is indicated with one-hot encoding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf6KRoOEyAnG"
      },
      "source": [
        "#The labels are divided for each dataset (training, validation and testing)\n",
        "\n",
        "Train_path = path[:int(num_community*0.6)]\n",
        "Validation_path = path[int(num_community*0.6):int(num_community*0.8)]\n",
        "Test_path = path[int(num_community*0.8):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8DupvQ2ydSL"
      },
      "source": [
        "#One-hot encoding of path length\n",
        "\n",
        "Label_Train = np.zeros((int(num_community*0.6),19))\n",
        "Label_Test = np.zeros((int(num_community*0.2),19))\n",
        "Label_Val = np.zeros((int(num_community*0.2),19))\n",
        "\n",
        "\n",
        "for j in range(0,len(Train_path)):\n",
        "    i=Train_path[j][0]\n",
        "    Label_Train[j][i-1] = 1\n",
        "\n",
        "for j in range(0,len(Validation_path)):\n",
        "    i=Validation_path[j][0]\n",
        "    Label_Val[j][i-1] = 1\n",
        "\n",
        "for j in range(0,len(Test_path)):\n",
        "    i=Test_path[j][0]\n",
        "    Label_Test[j][i-1] = 1\n",
        "    \n",
        "label_tv = np.concatenate((Label_Train, Label_Val)) \n",
        "labels = np.concatenate((label_tv, Label_Test)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZgqB6bix_6i"
      },
      "source": [
        "## Unlabeling\n",
        "\n",
        "Following the intended behavior of the Kipf model, it will be trained on both labeled and unlabeled examples (i.e. in the training phase the validation and test datasets will be unlabeled) and later will be evaluated the predictions of the trained model on previously unlabeled examples. \n",
        "\n",
        "In this setting, the model always performs a forward pass on the full dataset and it is required a mask that affects which labels are available to the model during the different phases. All other nodes are treated as **unlabeled** nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDnRk59I0qvW"
      },
      "source": [
        "def sample_mask(idx, l):                                    #Create a bolean array for every dataset (train, val, test) to use as a mask filter for the labels\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "train_size = int(num_community*0.6)                         #Different sizes of the different datasets\n",
        "val_size = int(num_community*0.2)\n",
        "test_size = int(num_community*0.2)\n",
        "\n",
        "\n",
        "idx_train = range(train_size)                               #Vectors containing the indexes of the labeled subgraphs\n",
        "idx_val = range(train_size, train_size+val_size)\n",
        "idx_test = range(train_size+val_size, train_size+val_size+test_size)\n",
        "\n",
        "\n",
        "train_mask = sample_mask(idx_train, labels.shape[0])        #Masks that are true where indicated by the indexes vectors, false otherwise\n",
        "val_mask = sample_mask(idx_val, labels.shape[0])\n",
        "test_mask = sample_mask(idx_test, labels.shape[0])\n",
        "\n",
        "\n",
        "y_train = np.zeros(labels.shape)                            #The datasets are all the same size\n",
        "y_val = np.zeros(labels.shape)\n",
        "y_test = np.zeros(labels.shape)\n",
        "y_train[train_mask, :] = labels[train_mask, :]              #But unlabeled where indicated by the mask\n",
        "y_val[val_mask, :] = labels[val_mask, :]\n",
        "y_test[test_mask, :] = labels[test_mask, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuWNhW4K11CJ"
      },
      "source": [
        "## Model initialization\n",
        "\n",
        "Now is time to define the morphology and the dictionary for the *Graph Convolutional Neural Network* that will be deployed for this application.\n",
        "\n",
        "The structure is exactly the same introduced by Kipf in 2017 but empirically it has been determined the internal structure with 2 hidden layers of 1024 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RaVxmce1yaT"
      },
      "source": [
        "adj = adj_sparse\n",
        "features = features_sparse   \n",
        "y_train = y_train \n",
        "y_val = y_val\n",
        "train_mask = train_mask\n",
        "val_mask = val_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdnzDYT2gdP"
      },
      "source": [
        "#Slightly modified version of Kipf 2017 Model\n",
        "\n",
        "class GCN(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(GCN, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        # Weight decay loss\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        # Cross entropy error\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                            dropout=True,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=FLAGS.hidden2,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,                      \n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "        \n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden2,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x,\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZtpAF-j3Yvb"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "As mentioned in the theoretical section of the paper associated to this notebook, the data need to be preprocessed before give it to the network following the normalization: \n",
        "\n",
        "<center>$\\widetilde D^{-\\frac{1}{2}} \\widetilde A \\widetilde D^{-\\frac{1}{2}}$</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DYkjsWF4LOs"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    def to_tuple(mx):\n",
        "        if not sp.sparse.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "    \n",
        "#The power of a diagonal matrix is the power of each component so D is generated from A summing all the rows, in this way every single component of the vector can be simply elevated\n",
        "\n",
        "def normalize_adj(adj): \n",
        "    adj = sp.sparse.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))                           #Sum every column cell for each row, in this direction ->. So the result is a vector with as many cell as the number of raws (35000 in this case)\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.sparse.diags(d_inv_sqrt, 0)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "\n",
        "#Simply following the formula A' is the sum of A+I\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    adj_normalized = normalize_adj(adj + sp.sparse.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "\n",
        "num_supports = 1\n",
        "model_func = GCN\n",
        "support = [preprocess_adj(adj)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a9Dx5rW4mh3"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    rowsum = np.array(features.sum(1))                      #Sum of the columns\n",
        "    r_inv1 = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv2 = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv = np.multiply(r_inv1, r_inv2)\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.sparse.diags(r_inv, 0)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "features = preprocess_features(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDFU9UwU5ABt",
        "outputId": "e504adbc-1d0b-44d1-e94e-a9fa02c6ef3f"
      },
      "source": [
        "features                                                    #In the first array are contained the adjacency matrices of every subgraph. \n",
        "                                                            #To every subgraph edge is indicated a weight that rappresents the total number of edges in the i-th subgraph."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[    0,   396],\n",
              "        [    0,   390],\n",
              "        [    0,   386],\n",
              "        ...,\n",
              "        [34999,     9],\n",
              "        [34999,     5],\n",
              "        [34999,     3]], dtype=int32),\n",
              " array([0.00892857, 0.00892857, 0.00892857, ..., 0.00892857, 0.00892857,\n",
              "        0.00892857]),\n",
              " (35000, 400))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwBseg7aANqD"
      },
      "source": [
        "## Training pipeline\n",
        "\n",
        "The title is pretty comprehensive. According to the Kipf design in this phase all the required variables are initialized and the model is trained using the datasets built and processed before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KiyMbh_5R3K",
        "outputId": "17943699-5cde-4430-ca15-7d1b0b84f853"
      },
      "source": [
        "#Set random seed\n",
        "\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "#Settings\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "#Definition of network and training parameters\n",
        "\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')  \n",
        "flags.DEFINE_float('learning_rate', 1e-3, 'Initial learning rate.')\n",
        "\n",
        "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 1024, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 1024, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "\n",
        "flags.DEFINE_integer('early_stopping', 300, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7fa9bba7c650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsUH1-C75h_3"
      },
      "source": [
        "def masked_softmax_cross_entropy(preds, labels, mask):                       #Softmax cross-entropy loss with masking\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_mean_square_error(preds,labels,mask):                             #L-2 loss\n",
        "    loss = tf.nn.l2_loss(preds - labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):                                    #Accuracy with masking\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaZv_9AP5aEe",
        "outputId": "31b3cbad-64fa-44b9-ea02-a95dc5e5d3f9"
      },
      "source": [
        "#Extraction of the network configuration parameters\n",
        "\n",
        "remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
        "assert(remaining_args == [sys.argv[0]])\n",
        "if (FLAGS.model == 'gcn'):\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)] \n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "\n",
        "\n",
        "#Definition of placeholders as provided by Kipf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#Declaration of the model\n",
        "\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "\n",
        "\n",
        "#Session initialization\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "\n",
        "\n",
        "#Definition of the model evaluation function\n",
        "\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVKlN18N5sHM"
      },
      "source": [
        "#Same as Kipf 2017, function to compile the training dictionary\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAXv7_as5ymi",
        "outputId": "ca51344f-9924-470f-d38f-4c420aac2be8"
      },
      "source": [
        "#Initialization of the session\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "train_accuracy = []\n",
        "validation_accuracy = []\n",
        "\n",
        "\n",
        "#Training\n",
        "\n",
        "for epoch in range(FLAGS.epochs):\n",
        "\n",
        "    t = time.time()                                                                         #Compilation of the dictionary\n",
        "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
        "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)        #Training steps\n",
        "    \n",
        "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)        #Validation steps\n",
        "    validation_loss.append(cost)\n",
        "    validation_accuracy.append(acc)\n",
        "    train_loss.append(outs[1])\n",
        "    train_accuracy.append(outs[2])\n",
        "\n",
        "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
        "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
        "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
        "\n",
        "    if epoch > FLAGS.early_stopping and validation_loss[-1] > np.mean(validation_loss[-(FLAGS.early_stopping+1):-1]):\n",
        "        print(\"Early stopping...\")\n",
        "        break\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 train_loss= 3.08878 train_acc= 0.04348 val_loss= 3.07392 val_acc= 0.30600 time= 6.08079\n",
            "Epoch: 0002 train_loss= 3.07144 train_acc= 0.29824 val_loss= 3.05899 val_acc= 0.30600 time= 5.71811\n",
            "Epoch: 0003 train_loss= 3.05377 train_acc= 0.29824 val_loss= 3.04165 val_acc= 0.30600 time= 5.64456\n",
            "Epoch: 0004 train_loss= 3.03398 train_acc= 0.29824 val_loss= 3.01995 val_acc= 0.30600 time= 5.68233\n",
            "Epoch: 0005 train_loss= 3.01027 train_acc= 0.29824 val_loss= 2.99190 val_acc= 0.30600 time= 5.63667\n",
            "Epoch: 0006 train_loss= 2.98128 train_acc= 0.29824 val_loss= 2.95561 val_acc= 0.30600 time= 5.69901\n",
            "Epoch: 0007 train_loss= 2.94450 train_acc= 0.29824 val_loss= 2.90982 val_acc= 0.30600 time= 5.67964\n",
            "Epoch: 0008 train_loss= 2.89970 train_acc= 0.29824 val_loss= 2.85468 val_acc= 0.30600 time= 5.65176\n",
            "Epoch: 0009 train_loss= 2.84699 train_acc= 0.29824 val_loss= 2.79320 val_acc= 0.30600 time= 5.70250\n",
            "Epoch: 0010 train_loss= 2.78958 train_acc= 0.29824 val_loss= 2.73334 val_acc= 0.30600 time= 5.68180\n",
            "Epoch: 0011 train_loss= 2.73737 train_acc= 0.29824 val_loss= 2.69112 val_acc= 0.30600 time= 5.69822\n",
            "Epoch: 0012 train_loss= 2.70222 train_acc= 0.29824 val_loss= 2.68797 val_acc= 0.30600 time= 5.66505\n",
            "Epoch: 0013 train_loss= 2.70968 train_acc= 0.29824 val_loss= 2.71568 val_acc= 0.30600 time= 5.65863\n",
            "Epoch: 0014 train_loss= 2.74510 train_acc= 0.29824 val_loss= 2.72621 val_acc= 0.30600 time= 5.62701\n",
            "Epoch: 0015 train_loss= 2.75410 train_acc= 0.29824 val_loss= 2.71034 val_acc= 0.30600 time= 5.65771\n",
            "Epoch: 0016 train_loss= 2.73692 train_acc= 0.29824 val_loss= 2.68502 val_acc= 0.30600 time= 5.65159\n",
            "Epoch: 0017 train_loss= 2.71021 train_acc= 0.29824 val_loss= 2.66536 val_acc= 0.30600 time= 5.65616\n",
            "Epoch: 0018 train_loss= 2.68520 train_acc= 0.29824 val_loss= 2.65727 val_acc= 0.30600 time= 5.67832\n",
            "Epoch: 0019 train_loss= 2.67359 train_acc= 0.29824 val_loss= 2.65886 val_acc= 0.30600 time= 5.63524\n",
            "Epoch: 0020 train_loss= 2.67080 train_acc= 0.29824 val_loss= 2.66504 val_acc= 0.30600 time= 5.63739\n",
            "Epoch: 0021 train_loss= 2.67516 train_acc= 0.29824 val_loss= 2.67116 val_acc= 0.30600 time= 5.59833\n",
            "Epoch: 0022 train_loss= 2.67915 train_acc= 0.29824 val_loss= 2.67433 val_acc= 0.30600 time= 5.63038\n",
            "Epoch: 0023 train_loss= 2.68072 train_acc= 0.29824 val_loss= 2.67334 val_acc= 0.30600 time= 5.60112\n",
            "Epoch: 0024 train_loss= 2.67955 train_acc= 0.29824 val_loss= 2.66809 val_acc= 0.30600 time= 5.61641\n",
            "Epoch: 0025 train_loss= 2.67582 train_acc= 0.29824 val_loss= 2.65936 val_acc= 0.30600 time= 5.65462\n",
            "Epoch: 0026 train_loss= 2.66975 train_acc= 0.29824 val_loss= 2.64857 val_acc= 0.30600 time= 5.62251\n",
            "Epoch: 0027 train_loss= 2.66011 train_acc= 0.29824 val_loss= 2.63746 val_acc= 0.30600 time= 5.59932\n",
            "Epoch: 0028 train_loss= 2.65157 train_acc= 0.29824 val_loss= 2.62791 val_acc= 0.30600 time= 5.61881\n",
            "Epoch: 0029 train_loss= 2.64620 train_acc= 0.29824 val_loss= 2.62135 val_acc= 0.30600 time= 5.64295\n",
            "Epoch: 0030 train_loss= 2.64066 train_acc= 0.29824 val_loss= 2.61800 val_acc= 0.30600 time= 5.58293\n",
            "Epoch: 0031 train_loss= 2.64039 train_acc= 0.29824 val_loss= 2.61656 val_acc= 0.30600 time= 5.61197\n",
            "Epoch: 0032 train_loss= 2.63996 train_acc= 0.29824 val_loss= 2.61488 val_acc= 0.30600 time= 5.57754\n",
            "Epoch: 0033 train_loss= 2.63950 train_acc= 0.29824 val_loss= 2.61153 val_acc= 0.30600 time= 5.59469\n",
            "Epoch: 0034 train_loss= 2.63676 train_acc= 0.29824 val_loss= 2.60671 val_acc= 0.30600 time= 5.63287\n",
            "Epoch: 0035 train_loss= 2.62863 train_acc= 0.29824 val_loss= 2.60185 val_acc= 0.30600 time= 5.62471\n",
            "Epoch: 0036 train_loss= 2.62428 train_acc= 0.29824 val_loss= 2.59827 val_acc= 0.30600 time= 5.62770\n",
            "Epoch: 0037 train_loss= 2.61958 train_acc= 0.29824 val_loss= 2.59637 val_acc= 0.30600 time= 5.63455\n",
            "Epoch: 0038 train_loss= 2.61585 train_acc= 0.29824 val_loss= 2.59554 val_acc= 0.30600 time= 5.59229\n",
            "Epoch: 0039 train_loss= 2.61290 train_acc= 0.29824 val_loss= 2.59469 val_acc= 0.30600 time= 5.60140\n",
            "Epoch: 0040 train_loss= 2.61141 train_acc= 0.29824 val_loss= 2.59275 val_acc= 0.30600 time= 5.57963\n",
            "Epoch: 0041 train_loss= 2.60578 train_acc= 0.29824 val_loss= 2.58912 val_acc= 0.30600 time= 5.59767\n",
            "Epoch: 0042 train_loss= 2.60374 train_acc= 0.29824 val_loss= 2.58386 val_acc= 0.30600 time= 5.64411\n",
            "Epoch: 0043 train_loss= 2.59990 train_acc= 0.29824 val_loss= 2.57761 val_acc= 0.30600 time= 5.59649\n",
            "Epoch: 0044 train_loss= 2.59596 train_acc= 0.29824 val_loss= 2.57135 val_acc= 0.30600 time= 5.65238\n",
            "Epoch: 0045 train_loss= 2.59010 train_acc= 0.29824 val_loss= 2.56588 val_acc= 0.30600 time= 5.64330\n",
            "Epoch: 0046 train_loss= 2.58607 train_acc= 0.29824 val_loss= 2.56146 val_acc= 0.30600 time= 5.59246\n",
            "Epoch: 0047 train_loss= 2.58568 train_acc= 0.29824 val_loss= 2.55764 val_acc= 0.30600 time= 5.63448\n",
            "Epoch: 0048 train_loss= 2.58140 train_acc= 0.29824 val_loss= 2.55368 val_acc= 0.30600 time= 5.57335\n",
            "Epoch: 0049 train_loss= 2.57731 train_acc= 0.29824 val_loss= 2.54932 val_acc= 0.30600 time= 5.60882\n",
            "Epoch: 0050 train_loss= 2.57305 train_acc= 0.29824 val_loss= 2.54497 val_acc= 0.30600 time= 5.65591\n",
            "Epoch: 0051 train_loss= 2.56900 train_acc= 0.29824 val_loss= 2.54119 val_acc= 0.30600 time= 5.56714\n",
            "Epoch: 0052 train_loss= 2.56366 train_acc= 0.29824 val_loss= 2.53806 val_acc= 0.30600 time= 5.63004\n",
            "Epoch: 0053 train_loss= 2.55896 train_acc= 0.29824 val_loss= 2.53502 val_acc= 0.30600 time= 5.57665\n",
            "Epoch: 0054 train_loss= 2.55511 train_acc= 0.29824 val_loss= 2.53133 val_acc= 0.30600 time= 5.58358\n",
            "Epoch: 0055 train_loss= 2.55228 train_acc= 0.29824 val_loss= 2.52649 val_acc= 0.30600 time= 5.58307\n",
            "Epoch: 0056 train_loss= 2.54845 train_acc= 0.29824 val_loss= 2.52061 val_acc= 0.30600 time= 5.58977\n",
            "Epoch: 0057 train_loss= 2.54344 train_acc= 0.29824 val_loss= 2.51436 val_acc= 0.30600 time= 5.58906\n",
            "Epoch: 0058 train_loss= 2.53890 train_acc= 0.29824 val_loss= 2.50839 val_acc= 0.30600 time= 5.58626\n",
            "Epoch: 0059 train_loss= 2.53276 train_acc= 0.29824 val_loss= 2.50286 val_acc= 0.30600 time= 5.62203\n",
            "Epoch: 0060 train_loss= 2.53170 train_acc= 0.29824 val_loss= 2.49751 val_acc= 0.30600 time= 5.80592\n",
            "Epoch: 0061 train_loss= 2.52476 train_acc= 0.29824 val_loss= 2.49207 val_acc= 0.30600 time= 5.65485\n",
            "Epoch: 0062 train_loss= 2.52204 train_acc= 0.29824 val_loss= 2.48677 val_acc= 0.30600 time= 5.63630\n",
            "Epoch: 0063 train_loss= 2.51408 train_acc= 0.29824 val_loss= 2.48186 val_acc= 0.30600 time= 5.66320\n",
            "Epoch: 0064 train_loss= 2.51084 train_acc= 0.29824 val_loss= 2.47712 val_acc= 0.30600 time= 5.65258\n",
            "Epoch: 0065 train_loss= 2.50766 train_acc= 0.29824 val_loss= 2.47183 val_acc= 0.30600 time= 5.63502\n",
            "Epoch: 0066 train_loss= 2.50081 train_acc= 0.29824 val_loss= 2.46546 val_acc= 0.30600 time= 5.63309\n",
            "Epoch: 0067 train_loss= 2.49710 train_acc= 0.29824 val_loss= 2.45823 val_acc= 0.30600 time= 5.63787\n",
            "Epoch: 0068 train_loss= 2.49411 train_acc= 0.29824 val_loss= 2.45089 val_acc= 0.30600 time= 5.68230\n",
            "Epoch: 0069 train_loss= 2.48447 train_acc= 0.29824 val_loss= 2.44394 val_acc= 0.30600 time= 5.60863\n",
            "Epoch: 0070 train_loss= 2.48218 train_acc= 0.29824 val_loss= 2.43725 val_acc= 0.30600 time= 5.62678\n",
            "Epoch: 0071 train_loss= 2.47639 train_acc= 0.29824 val_loss= 2.43084 val_acc= 0.30600 time= 5.73445\n",
            "Epoch: 0072 train_loss= 2.47201 train_acc= 0.29824 val_loss= 2.42496 val_acc= 0.30600 time= 5.70599\n",
            "Epoch: 0073 train_loss= 2.46881 train_acc= 0.29824 val_loss= 2.41904 val_acc= 0.30600 time= 5.68078\n",
            "Epoch: 0074 train_loss= 2.46175 train_acc= 0.29824 val_loss= 2.41217 val_acc= 0.30600 time= 5.70809\n",
            "Epoch: 0075 train_loss= 2.45342 train_acc= 0.29824 val_loss= 2.40422 val_acc= 0.30600 time= 5.69710\n",
            "Epoch: 0076 train_loss= 2.44830 train_acc= 0.29824 val_loss= 2.39595 val_acc= 0.30600 time= 5.65722\n",
            "Epoch: 0077 train_loss= 2.44403 train_acc= 0.29824 val_loss= 2.38809 val_acc= 0.30600 time= 5.73039\n",
            "Epoch: 0078 train_loss= 2.43838 train_acc= 0.29824 val_loss= 2.38066 val_acc= 0.30600 time= 5.75791\n",
            "Epoch: 0079 train_loss= 2.43499 train_acc= 0.29824 val_loss= 2.37357 val_acc= 0.30600 time= 5.77049\n",
            "Epoch: 0080 train_loss= 2.42898 train_acc= 0.29824 val_loss= 2.36667 val_acc= 0.30600 time= 5.67923\n",
            "Epoch: 0081 train_loss= 2.42414 train_acc= 0.29824 val_loss= 2.35935 val_acc= 0.30600 time= 5.66338\n",
            "Epoch: 0082 train_loss= 2.41651 train_acc= 0.29824 val_loss= 2.35119 val_acc= 0.30600 time= 5.75560\n",
            "Epoch: 0083 train_loss= 2.41078 train_acc= 0.29824 val_loss= 2.34250 val_acc= 0.30600 time= 5.71444\n",
            "Epoch: 0084 train_loss= 2.40566 train_acc= 0.29824 val_loss= 2.33408 val_acc= 0.30600 time= 5.75851\n",
            "Epoch: 0085 train_loss= 2.39857 train_acc= 0.29824 val_loss= 2.32637 val_acc= 0.30600 time= 5.70146\n",
            "Epoch: 0086 train_loss= 2.39020 train_acc= 0.29824 val_loss= 2.31909 val_acc= 0.30600 time= 5.63728\n",
            "Epoch: 0087 train_loss= 2.39063 train_acc= 0.29824 val_loss= 2.31113 val_acc= 0.30600 time= 5.86798\n",
            "Epoch: 0088 train_loss= 2.38212 train_acc= 0.29824 val_loss= 2.30276 val_acc= 0.30600 time= 7.46811\n",
            "Epoch: 0089 train_loss= 2.37635 train_acc= 0.29829 val_loss= 2.29403 val_acc= 0.30600 time= 6.18635\n",
            "Epoch: 0090 train_loss= 2.37358 train_acc= 0.29824 val_loss= 2.28569 val_acc= 0.30600 time= 5.66987\n",
            "Epoch: 0091 train_loss= 2.36519 train_acc= 0.29824 val_loss= 2.27794 val_acc= 0.30600 time= 5.73717\n",
            "Epoch: 0092 train_loss= 2.35394 train_acc= 0.29838 val_loss= 2.27000 val_acc= 0.30600 time= 5.68189\n",
            "Epoch: 0093 train_loss= 2.35306 train_acc= 0.29838 val_loss= 2.26153 val_acc= 0.30600 time= 5.69034\n",
            "Epoch: 0094 train_loss= 2.34619 train_acc= 0.29867 val_loss= 2.25281 val_acc= 0.30600 time= 5.71970\n",
            "Epoch: 0095 train_loss= 2.34783 train_acc= 0.29914 val_loss= 2.24462 val_acc= 0.30600 time= 5.68128\n",
            "Epoch: 0096 train_loss= 2.34049 train_acc= 0.29957 val_loss= 2.23684 val_acc= 0.30600 time= 5.69209\n",
            "Epoch: 0097 train_loss= 2.33383 train_acc= 0.30033 val_loss= 2.22893 val_acc= 0.30629 time= 5.73775\n",
            "Epoch: 0098 train_loss= 2.32586 train_acc= 0.30200 val_loss= 2.22056 val_acc= 0.30757 time= 5.66511\n",
            "Epoch: 0099 train_loss= 2.32079 train_acc= 0.30324 val_loss= 2.21195 val_acc= 0.30857 time= 5.70004\n",
            "Epoch: 0100 train_loss= 2.31649 train_acc= 0.30529 val_loss= 2.20400 val_acc= 0.31300 time= 5.67160\n",
            "Epoch: 0101 train_loss= 2.31785 train_acc= 0.30700 val_loss= 2.19657 val_acc= 0.32043 time= 5.67167\n",
            "Epoch: 0102 train_loss= 2.30939 train_acc= 0.31229 val_loss= 2.18881 val_acc= 0.32943 time= 5.70976\n",
            "Epoch: 0103 train_loss= 2.30430 train_acc= 0.31762 val_loss= 2.18064 val_acc= 0.33443 time= 5.68765\n",
            "Epoch: 0104 train_loss= 2.30234 train_acc= 0.31852 val_loss= 2.17298 val_acc= 0.33957 time= 5.66948\n",
            "Epoch: 0105 train_loss= 2.29356 train_acc= 0.32148 val_loss= 2.16553 val_acc= 0.34886 time= 5.73148\n",
            "Epoch: 0106 train_loss= 2.28572 train_acc= 0.32371 val_loss= 2.15847 val_acc= 0.36100 time= 5.69024\n",
            "Epoch: 0107 train_loss= 2.27893 train_acc= 0.32867 val_loss= 2.15098 val_acc= 0.36614 time= 5.71572\n",
            "Epoch: 0108 train_loss= 2.27919 train_acc= 0.33119 val_loss= 2.14354 val_acc= 0.36414 time= 5.64487\n",
            "Epoch: 0109 train_loss= 2.27895 train_acc= 0.32800 val_loss= 2.13659 val_acc= 0.36600 time= 5.66652\n",
            "Epoch: 0110 train_loss= 2.26695 train_acc= 0.33500 val_loss= 2.12956 val_acc= 0.37814 time= 5.71877\n",
            "Epoch: 0111 train_loss= 2.26735 train_acc= 0.33386 val_loss= 2.12275 val_acc= 0.38729 time= 5.70598\n",
            "Epoch: 0112 train_loss= 2.26539 train_acc= 0.34024 val_loss= 2.11598 val_acc= 0.38286 time= 5.73825\n",
            "Epoch: 0113 train_loss= 2.26464 train_acc= 0.33624 val_loss= 2.10972 val_acc= 0.38257 time= 5.73767\n",
            "Epoch: 0114 train_loss= 2.25325 train_acc= 0.33933 val_loss= 2.10260 val_acc= 0.39314 time= 5.66612\n",
            "Epoch: 0115 train_loss= 2.25267 train_acc= 0.34062 val_loss= 2.09598 val_acc= 0.40143 time= 5.71471\n",
            "Epoch: 0116 train_loss= 2.24049 train_acc= 0.34481 val_loss= 2.08958 val_acc= 0.40157 time= 5.72895\n",
            "Epoch: 0117 train_loss= 2.23965 train_acc= 0.34667 val_loss= 2.08429 val_acc= 0.39571 time= 5.66929\n",
            "Epoch: 0118 train_loss= 2.23882 train_acc= 0.34657 val_loss= 2.07790 val_acc= 0.39886 time= 5.80355\n",
            "Epoch: 0119 train_loss= 2.23898 train_acc= 0.34790 val_loss= 2.07061 val_acc= 0.40986 time= 5.78828\n",
            "Epoch: 0120 train_loss= 2.24077 train_acc= 0.35238 val_loss= 2.06471 val_acc= 0.40929 time= 5.76539\n",
            "Epoch: 0121 train_loss= 2.22916 train_acc= 0.35452 val_loss= 2.06006 val_acc= 0.40529 time= 5.68283\n",
            "Epoch: 0122 train_loss= 2.21461 train_acc= 0.35557 val_loss= 2.05376 val_acc= 0.40971 time= 5.65674\n",
            "Epoch: 0123 train_loss= 2.22159 train_acc= 0.35586 val_loss= 2.04673 val_acc= 0.42100 time= 5.69174\n",
            "Epoch: 0124 train_loss= 2.21050 train_acc= 0.36181 val_loss= 2.04098 val_acc= 0.42529 time= 5.68243\n",
            "Epoch: 0125 train_loss= 2.20557 train_acc= 0.36710 val_loss= 2.03638 val_acc= 0.42200 time= 5.76372\n",
            "Epoch: 0126 train_loss= 2.20267 train_acc= 0.36714 val_loss= 2.03130 val_acc= 0.42414 time= 5.73582\n",
            "Epoch: 0127 train_loss= 2.20165 train_acc= 0.36552 val_loss= 2.02424 val_acc= 0.44000 time= 5.70417\n",
            "Epoch: 0128 train_loss= 2.20426 train_acc= 0.36667 val_loss= 2.01937 val_acc= 0.44043 time= 5.78065\n",
            "Epoch: 0129 train_loss= 2.19059 train_acc= 0.37248 val_loss= 2.01414 val_acc= 0.44386 time= 5.75135\n",
            "Epoch: 0130 train_loss= 2.19571 train_acc= 0.37033 val_loss= 2.00807 val_acc= 0.45286 time= 5.70562\n",
            "Epoch: 0131 train_loss= 2.18686 train_acc= 0.37471 val_loss= 2.00259 val_acc= 0.46043 time= 5.72164\n",
            "Epoch: 0132 train_loss= 2.17285 train_acc= 0.37962 val_loss= 1.99885 val_acc= 0.45329 time= 5.71690\n",
            "Epoch: 0133 train_loss= 2.17888 train_acc= 0.37871 val_loss= 1.99382 val_acc= 0.45871 time= 5.71220\n",
            "Epoch: 0134 train_loss= 2.17535 train_acc= 0.37790 val_loss= 1.98851 val_acc= 0.46586 time= 5.74003\n",
            "Epoch: 0135 train_loss= 2.18533 train_acc= 0.37862 val_loss= 1.98373 val_acc= 0.46886 time= 5.66159\n",
            "Epoch: 0136 train_loss= 2.17178 train_acc= 0.37990 val_loss= 1.97966 val_acc= 0.46943 time= 5.68009\n",
            "Epoch: 0137 train_loss= 2.16399 train_acc= 0.38295 val_loss= 1.97552 val_acc= 0.46914 time= 5.59108\n",
            "Epoch: 0138 train_loss= 2.16614 train_acc= 0.38290 val_loss= 1.97032 val_acc= 0.47314 time= 5.61420\n",
            "Epoch: 0139 train_loss= 2.16689 train_acc= 0.38395 val_loss= 1.96528 val_acc= 0.47771 time= 5.63767\n",
            "Epoch: 0140 train_loss= 2.16114 train_acc= 0.38710 val_loss= 1.96081 val_acc= 0.47986 time= 5.71176\n",
            "Epoch: 0141 train_loss= 2.14791 train_acc= 0.38938 val_loss= 1.95862 val_acc= 0.47343 time= 5.81086\n",
            "Epoch: 0142 train_loss= 2.16455 train_acc= 0.38210 val_loss= 1.95475 val_acc= 0.47429 time= 5.71088\n",
            "Epoch: 0143 train_loss= 2.16098 train_acc= 0.38167 val_loss= 1.94896 val_acc= 0.48257 time= 5.71057\n",
            "Epoch: 0144 train_loss= 2.15952 train_acc= 0.38324 val_loss= 1.94589 val_acc= 0.48200 time= 5.82167\n",
            "Epoch: 0145 train_loss= 2.15269 train_acc= 0.38581 val_loss= 1.94355 val_acc= 0.47929 time= 5.74616\n",
            "Epoch: 0146 train_loss= 2.15540 train_acc= 0.38500 val_loss= 1.94028 val_acc= 0.47857 time= 5.64626\n",
            "Epoch: 0147 train_loss= 2.14692 train_acc= 0.38690 val_loss= 1.93490 val_acc= 0.48600 time= 5.69105\n",
            "Epoch: 0148 train_loss= 2.14666 train_acc= 0.38943 val_loss= 1.93100 val_acc= 0.48743 time= 5.69169\n",
            "Epoch: 0149 train_loss= 2.13469 train_acc= 0.38867 val_loss= 1.92940 val_acc= 0.48229 time= 5.68476\n",
            "Epoch: 0150 train_loss= 2.14149 train_acc= 0.38662 val_loss= 1.92704 val_acc= 0.48114 time= 5.69040\n",
            "Epoch: 0151 train_loss= 2.13240 train_acc= 0.39010 val_loss= 1.92219 val_acc= 0.48957 time= 5.67552\n",
            "Epoch: 0152 train_loss= 2.13450 train_acc= 0.39376 val_loss= 1.91882 val_acc= 0.49157 time= 5.71676\n",
            "Epoch: 0153 train_loss= 2.12701 train_acc= 0.39443 val_loss= 1.91689 val_acc= 0.48786 time= 5.68204\n",
            "Epoch: 0154 train_loss= 2.12928 train_acc= 0.39214 val_loss= 1.91318 val_acc= 0.49286 time= 5.67550\n",
            "Epoch: 0155 train_loss= 2.13008 train_acc= 0.39143 val_loss= 1.91063 val_acc= 0.49271 time= 5.81446\n",
            "Epoch: 0156 train_loss= 2.12413 train_acc= 0.39633 val_loss= 1.90784 val_acc= 0.49214 time= 5.66569\n",
            "Epoch: 0157 train_loss= 2.12069 train_acc= 0.39200 val_loss= 1.90447 val_acc= 0.49714 time= 5.77010\n",
            "Epoch: 0158 train_loss= 2.11984 train_acc= 0.39395 val_loss= 1.90091 val_acc= 0.50043 time= 5.73033\n",
            "Epoch: 0159 train_loss= 2.12521 train_acc= 0.39195 val_loss= 1.89863 val_acc= 0.49814 time= 5.66127\n",
            "Epoch: 0160 train_loss= 2.12604 train_acc= 0.39319 val_loss= 1.89565 val_acc= 0.50143 time= 5.73379\n",
            "Epoch: 0161 train_loss= 2.10842 train_acc= 0.39833 val_loss= 1.89183 val_acc= 0.50643 time= 5.74640\n",
            "Epoch: 0162 train_loss= 2.11089 train_acc= 0.40233 val_loss= 1.88963 val_acc= 0.50614 time= 5.78279\n",
            "Epoch: 0163 train_loss= 2.10935 train_acc= 0.40119 val_loss= 1.88773 val_acc= 0.50557 time= 5.74208\n",
            "Epoch: 0164 train_loss= 2.11006 train_acc= 0.39848 val_loss= 1.88472 val_acc= 0.50814 time= 5.74028\n",
            "Epoch: 0165 train_loss= 2.10864 train_acc= 0.40281 val_loss= 1.88004 val_acc= 0.51443 time= 5.74817\n",
            "Epoch: 0166 train_loss= 2.10167 train_acc= 0.40476 val_loss= 1.87694 val_acc= 0.51486 time= 5.69148\n",
            "Epoch: 0167 train_loss= 2.10066 train_acc= 0.40400 val_loss= 1.87396 val_acc= 0.51514 time= 5.71121\n",
            "Epoch: 0168 train_loss= 2.09389 train_acc= 0.40771 val_loss= 1.87174 val_acc= 0.51414 time= 5.73722\n",
            "Epoch: 0169 train_loss= 2.09882 train_acc= 0.40433 val_loss= 1.87104 val_acc= 0.50800 time= 5.72785\n",
            "Epoch: 0170 train_loss= 2.10300 train_acc= 0.40076 val_loss= 1.86747 val_acc= 0.51443 time= 5.62861\n",
            "Epoch: 0171 train_loss= 2.09637 train_acc= 0.40319 val_loss= 1.86306 val_acc= 0.52257 time= 5.71250\n",
            "Epoch: 0172 train_loss= 2.09354 train_acc= 0.40367 val_loss= 1.86045 val_acc= 0.52386 time= 5.79882\n",
            "Epoch: 0173 train_loss= 2.09121 train_acc= 0.40705 val_loss= 1.85971 val_acc= 0.51886 time= 5.78011\n",
            "Epoch: 0174 train_loss= 2.09163 train_acc= 0.40495 val_loss= 1.85867 val_acc= 0.51414 time= 5.79497\n",
            "Epoch: 0175 train_loss= 2.08684 train_acc= 0.40467 val_loss= 1.85461 val_acc= 0.52171 time= 5.82392\n",
            "Epoch: 0176 train_loss= 2.08331 train_acc= 0.40662 val_loss= 1.84979 val_acc= 0.53186 time= 5.80593\n",
            "Epoch: 0177 train_loss= 2.09585 train_acc= 0.40710 val_loss= 1.84875 val_acc= 0.52700 time= 5.76390\n",
            "Epoch: 0178 train_loss= 2.08792 train_acc= 0.40800 val_loss= 1.84773 val_acc= 0.52229 time= 5.87206\n",
            "Epoch: 0179 train_loss= 2.08139 train_acc= 0.41033 val_loss= 1.84303 val_acc= 0.53171 time= 5.80386\n",
            "Epoch: 0180 train_loss= 2.08264 train_acc= 0.40971 val_loss= 1.83945 val_acc= 0.53729 time= 5.75897\n",
            "Epoch: 0181 train_loss= 2.08185 train_acc= 0.40729 val_loss= 1.83756 val_acc= 0.53829 time= 5.82239\n",
            "Epoch: 0182 train_loss= 2.07939 train_acc= 0.41048 val_loss= 1.83910 val_acc= 0.52471 time= 5.76964\n",
            "Epoch: 0183 train_loss= 2.07824 train_acc= 0.40986 val_loss= 1.83507 val_acc= 0.53471 time= 5.74375\n",
            "Epoch: 0184 train_loss= 2.06672 train_acc= 0.41367 val_loss= 1.83012 val_acc= 0.54771 time= 5.71466\n",
            "Epoch: 0185 train_loss= 2.07386 train_acc= 0.41619 val_loss= 1.82928 val_acc= 0.54400 time= 5.72402\n",
            "Epoch: 0186 train_loss= 2.07712 train_acc= 0.41286 val_loss= 1.82998 val_acc= 0.53386 time= 5.83932\n",
            "Epoch: 0187 train_loss= 2.06772 train_acc= 0.41619 val_loss= 1.82539 val_acc= 0.54157 time= 5.71685\n",
            "Epoch: 0188 train_loss= 2.06976 train_acc= 0.41557 val_loss= 1.82153 val_acc= 0.54700 time= 5.74957\n",
            "Epoch: 0189 train_loss= 2.06965 train_acc= 0.41562 val_loss= 1.81927 val_acc= 0.54986 time= 5.76112\n",
            "Epoch: 0190 train_loss= 2.06977 train_acc= 0.41319 val_loss= 1.81969 val_acc= 0.54014 time= 5.69976\n",
            "Epoch: 0191 train_loss= 2.06040 train_acc= 0.41695 val_loss= 1.81709 val_acc= 0.54529 time= 5.73212\n",
            "Epoch: 0192 train_loss= 2.06631 train_acc= 0.41476 val_loss= 1.81405 val_acc= 0.55071 time= 5.68417\n",
            "Epoch: 0193 train_loss= 2.05696 train_acc= 0.41833 val_loss= 1.81260 val_acc= 0.55000 time= 5.67486\n",
            "Epoch: 0194 train_loss= 2.06792 train_acc= 0.41690 val_loss= 1.81166 val_acc= 0.54714 time= 5.73428\n",
            "Epoch: 0195 train_loss= 2.06748 train_acc= 0.41295 val_loss= 1.80825 val_acc= 0.55243 time= 5.70744\n",
            "Epoch: 0196 train_loss= 2.06112 train_acc= 0.41833 val_loss= 1.80414 val_acc= 0.55986 time= 5.64686\n",
            "Epoch: 0197 train_loss= 2.06727 train_acc= 0.41533 val_loss= 1.80438 val_acc= 0.55300 time= 5.72567\n",
            "Epoch: 0198 train_loss= 2.05653 train_acc= 0.41695 val_loss= 1.80503 val_acc= 0.54300 time= 5.65666\n",
            "Epoch: 0199 train_loss= 2.06231 train_acc= 0.41348 val_loss= 1.80103 val_acc= 0.55329 time= 5.74544\n",
            "Epoch: 0200 train_loss= 2.05513 train_acc= 0.41738 val_loss= 1.79764 val_acc= 0.55886 time= 5.69693\n",
            "Epoch: 0201 train_loss= 2.05676 train_acc= 0.41886 val_loss= 1.79722 val_acc= 0.55529 time= 5.63222\n",
            "Epoch: 0202 train_loss= 2.05274 train_acc= 0.42067 val_loss= 1.79755 val_acc= 0.54857 time= 5.70347\n",
            "Epoch: 0203 train_loss= 2.06030 train_acc= 0.41652 val_loss= 1.79426 val_acc= 0.55771 time= 5.65149\n",
            "Epoch: 0204 train_loss= 2.04902 train_acc= 0.41857 val_loss= 1.79173 val_acc= 0.56214 time= 5.69368\n",
            "Epoch: 0205 train_loss= 2.05102 train_acc= 0.41795 val_loss= 1.79181 val_acc= 0.55414 time= 5.66057\n",
            "Epoch: 0206 train_loss= 2.04960 train_acc= 0.41829 val_loss= 1.79085 val_acc= 0.55171 time= 5.70366\n",
            "Epoch: 0207 train_loss= 2.04003 train_acc= 0.41819 val_loss= 1.78831 val_acc= 0.55500 time= 5.64713\n",
            "Epoch: 0208 train_loss= 2.04459 train_acc= 0.42533 val_loss= 1.78511 val_acc= 0.56314 time= 5.61867\n",
            "Epoch: 0209 train_loss= 2.04526 train_acc= 0.42271 val_loss= 1.78381 val_acc= 0.56200 time= 5.65696\n",
            "Epoch: 0210 train_loss= 2.04490 train_acc= 0.41919 val_loss= 1.78462 val_acc= 0.55414 time= 5.72111\n",
            "Epoch: 0211 train_loss= 2.04581 train_acc= 0.41752 val_loss= 1.78250 val_acc= 0.55571 time= 5.69740\n",
            "Epoch: 0212 train_loss= 2.04441 train_acc= 0.41890 val_loss= 1.77882 val_acc= 0.56314 time= 5.66055\n",
            "Epoch: 0213 train_loss= 2.04437 train_acc= 0.42386 val_loss= 1.77806 val_acc= 0.56286 time= 5.66802\n",
            "Epoch: 0214 train_loss= 2.04094 train_acc= 0.41986 val_loss= 1.77808 val_acc= 0.55886 time= 5.66622\n",
            "Epoch: 0215 train_loss= 2.04506 train_acc= 0.42167 val_loss= 1.77577 val_acc= 0.56171 time= 5.63994\n",
            "Epoch: 0216 train_loss= 2.04000 train_acc= 0.41905 val_loss= 1.77255 val_acc= 0.57186 time= 5.66241\n",
            "Epoch: 0217 train_loss= 2.03048 train_acc= 0.42686 val_loss= 1.77094 val_acc= 0.57286 time= 5.66907\n",
            "Epoch: 0218 train_loss= 2.03518 train_acc= 0.42338 val_loss= 1.77047 val_acc= 0.57100 time= 5.70223\n",
            "Epoch: 0219 train_loss= 2.04259 train_acc= 0.42014 val_loss= 1.77062 val_acc= 0.56471 time= 5.64741\n",
            "Epoch: 0220 train_loss= 2.03577 train_acc= 0.42014 val_loss= 1.76975 val_acc= 0.56429 time= 5.76571\n",
            "Epoch: 0221 train_loss= 2.03207 train_acc= 0.42019 val_loss= 1.76481 val_acc= 0.57600 time= 5.71963\n",
            "Epoch: 0222 train_loss= 2.03283 train_acc= 0.42410 val_loss= 1.76373 val_acc= 0.57457 time= 5.66643\n",
            "Epoch: 0223 train_loss= 2.03764 train_acc= 0.42310 val_loss= 1.76557 val_acc= 0.56700 time= 5.69710\n",
            "Epoch: 0224 train_loss= 2.02931 train_acc= 0.42157 val_loss= 1.76270 val_acc= 0.57257 time= 5.70077\n",
            "Epoch: 0225 train_loss= 2.02944 train_acc= 0.42567 val_loss= 1.75738 val_acc= 0.58143 time= 5.76867\n",
            "Epoch: 0226 train_loss= 2.02984 train_acc= 0.42862 val_loss= 1.75697 val_acc= 0.57714 time= 5.71951\n",
            "Epoch: 0227 train_loss= 2.02678 train_acc= 0.42924 val_loss= 1.75879 val_acc= 0.56600 time= 5.65005\n",
            "Epoch: 0228 train_loss= 2.01843 train_acc= 0.42319 val_loss= 1.75605 val_acc= 0.56857 time= 5.72334\n",
            "Epoch: 0229 train_loss= 2.02050 train_acc= 0.42538 val_loss= 1.75105 val_acc= 0.57814 time= 5.73656\n",
            "Epoch: 0230 train_loss= 2.02908 train_acc= 0.42500 val_loss= 1.74944 val_acc= 0.58086 time= 5.65828\n",
            "Epoch: 0231 train_loss= 2.02973 train_acc= 0.42305 val_loss= 1.75050 val_acc= 0.57629 time= 5.64843\n",
            "Epoch: 0232 train_loss= 2.02068 train_acc= 0.42543 val_loss= 1.75050 val_acc= 0.57457 time= 5.65990\n",
            "Epoch: 0233 train_loss= 2.02661 train_acc= 0.42495 val_loss= 1.74873 val_acc= 0.57629 time= 5.61735\n",
            "Epoch: 0234 train_loss= 2.01668 train_acc= 0.42843 val_loss= 1.74585 val_acc= 0.57871 time= 5.61646\n",
            "Epoch: 0235 train_loss= 2.01604 train_acc= 0.42938 val_loss= 1.74469 val_acc= 0.57614 time= 5.64655\n",
            "Epoch: 0236 train_loss= 2.02380 train_acc= 0.42457 val_loss= 1.74436 val_acc= 0.57257 time= 5.66632\n",
            "Epoch: 0237 train_loss= 2.01124 train_acc= 0.42690 val_loss= 1.74166 val_acc= 0.57957 time= 5.64539\n",
            "Epoch: 0238 train_loss= 2.00822 train_acc= 0.43086 val_loss= 1.74000 val_acc= 0.58071 time= 5.60422\n",
            "Epoch: 0239 train_loss= 2.03013 train_acc= 0.42100 val_loss= 1.73954 val_acc= 0.57871 time= 5.70667\n",
            "Epoch: 0240 train_loss= 2.01505 train_acc= 0.42529 val_loss= 1.73700 val_acc= 0.58114 time= 5.64713\n",
            "Epoch: 0241 train_loss= 2.01526 train_acc= 0.42724 val_loss= 1.73623 val_acc= 0.58043 time= 5.72624\n",
            "Epoch: 0242 train_loss= 2.02397 train_acc= 0.42662 val_loss= 1.73774 val_acc= 0.57129 time= 5.71932\n",
            "Epoch: 0243 train_loss= 2.01462 train_acc= 0.42762 val_loss= 1.73473 val_acc= 0.57329 time= 5.70479\n",
            "Epoch: 0244 train_loss= 2.01176 train_acc= 0.42857 val_loss= 1.73032 val_acc= 0.58543 time= 5.69723\n",
            "Epoch: 0245 train_loss= 2.00831 train_acc= 0.43214 val_loss= 1.72919 val_acc= 0.58571 time= 5.67283\n",
            "Epoch: 0246 train_loss= 2.00078 train_acc= 0.43024 val_loss= 1.73123 val_acc= 0.57600 time= 5.70631\n",
            "Epoch: 0247 train_loss= 1.99457 train_acc= 0.43505 val_loss= 1.72833 val_acc= 0.58314 time= 5.68474\n",
            "Epoch: 0248 train_loss= 2.01086 train_acc= 0.42995 val_loss= 1.72495 val_acc= 0.58586 time= 5.65788\n",
            "Epoch: 0249 train_loss= 2.00677 train_acc= 0.43310 val_loss= 1.72411 val_acc= 0.58471 time= 5.71527\n",
            "Epoch: 0250 train_loss= 2.00783 train_acc= 0.42852 val_loss= 1.72487 val_acc= 0.57786 time= 5.73270\n",
            "Epoch: 0251 train_loss= 2.00142 train_acc= 0.43076 val_loss= 1.72375 val_acc= 0.57829 time= 5.66647\n",
            "Epoch: 0252 train_loss= 1.99217 train_acc= 0.43124 val_loss= 1.72029 val_acc= 0.58543 time= 5.64163\n",
            "Epoch: 0253 train_loss= 2.00117 train_acc= 0.43429 val_loss= 1.71732 val_acc= 0.59086 time= 5.65428\n",
            "Epoch: 0254 train_loss= 1.99686 train_acc= 0.43786 val_loss= 1.71949 val_acc= 0.58214 time= 5.60262\n",
            "Epoch: 0255 train_loss= 2.00469 train_acc= 0.42962 val_loss= 1.72178 val_acc= 0.57214 time= 5.69360\n",
            "Epoch: 0256 train_loss= 1.99526 train_acc= 0.42986 val_loss= 1.71618 val_acc= 0.58714 time= 5.67209\n",
            "Epoch: 0257 train_loss= 2.00120 train_acc= 0.43019 val_loss= 1.71320 val_acc= 0.59386 time= 5.68312\n",
            "Epoch: 0258 train_loss= 1.99973 train_acc= 0.43391 val_loss= 1.71464 val_acc= 0.58543 time= 5.65345\n",
            "Epoch: 0259 train_loss= 2.00190 train_acc= 0.43043 val_loss= 1.71443 val_acc= 0.58086 time= 5.61028\n",
            "Epoch: 0260 train_loss= 2.00366 train_acc= 0.42767 val_loss= 1.71139 val_acc= 0.58400 time= 5.70067\n",
            "Epoch: 0261 train_loss= 2.00532 train_acc= 0.42876 val_loss= 1.70914 val_acc= 0.58629 time= 5.61852\n",
            "Epoch: 0262 train_loss= 1.99888 train_acc= 0.43167 val_loss= 1.70847 val_acc= 0.58814 time= 5.69242\n",
            "Epoch: 0263 train_loss= 1.99256 train_acc= 0.43300 val_loss= 1.70828 val_acc= 0.58814 time= 5.71883\n",
            "Epoch: 0264 train_loss= 1.99374 train_acc= 0.43491 val_loss= 1.70710 val_acc= 0.58971 time= 5.66609\n",
            "Epoch: 0265 train_loss= 1.99454 train_acc= 0.43124 val_loss= 1.70374 val_acc= 0.59871 time= 5.78174\n",
            "Epoch: 0266 train_loss= 1.99116 train_acc= 0.43610 val_loss= 1.70323 val_acc= 0.59514 time= 5.69064\n",
            "Epoch: 0267 train_loss= 1.98549 train_acc= 0.43957 val_loss= 1.70444 val_acc= 0.58700 time= 5.72387\n",
            "Epoch: 0268 train_loss= 1.99439 train_acc= 0.42886 val_loss= 1.70486 val_acc= 0.58286 time= 5.71911\n",
            "Epoch: 0269 train_loss= 1.98573 train_acc= 0.43391 val_loss= 1.69786 val_acc= 0.59657 time= 5.68007\n",
            "Epoch: 0270 train_loss= 1.98959 train_acc= 0.43491 val_loss= 1.69550 val_acc= 0.59900 time= 5.67539\n",
            "Epoch: 0271 train_loss= 1.97654 train_acc= 0.44005 val_loss= 1.69482 val_acc= 0.59557 time= 6.19723\n",
            "Epoch: 0272 train_loss= 1.99720 train_acc= 0.43448 val_loss= 1.69746 val_acc= 0.58543 time= 5.69116\n",
            "Epoch: 0273 train_loss= 1.98194 train_acc= 0.43624 val_loss= 1.69370 val_acc= 0.59214 time= 5.73254\n",
            "Epoch: 0274 train_loss= 1.98090 train_acc= 0.43605 val_loss= 1.68949 val_acc= 0.60114 time= 5.68958\n",
            "Epoch: 0275 train_loss= 1.97792 train_acc= 0.43638 val_loss= 1.68762 val_acc= 0.60457 time= 5.67031\n",
            "Epoch: 0276 train_loss= 1.97908 train_acc= 0.43886 val_loss= 1.68976 val_acc= 0.59429 time= 5.72487\n",
            "Epoch: 0277 train_loss= 1.98067 train_acc= 0.43595 val_loss= 1.69012 val_acc= 0.59143 time= 5.75497\n",
            "Epoch: 0278 train_loss= 1.98161 train_acc= 0.43743 val_loss= 1.68725 val_acc= 0.59814 time= 5.82814\n",
            "Epoch: 0279 train_loss= 1.98355 train_acc= 0.43776 val_loss= 1.68364 val_acc= 0.60429 time= 5.84593\n",
            "Epoch: 0280 train_loss= 1.98900 train_acc= 0.43833 val_loss= 1.68329 val_acc= 0.60729 time= 5.74156\n",
            "Epoch: 0281 train_loss= 1.97636 train_acc= 0.43910 val_loss= 1.68379 val_acc= 0.60400 time= 5.80692\n",
            "Epoch: 0282 train_loss= 1.97164 train_acc= 0.43919 val_loss= 1.68179 val_acc= 0.60586 time= 5.72288\n",
            "Epoch: 0283 train_loss= 1.97623 train_acc= 0.43952 val_loss= 1.68058 val_acc= 0.60429 time= 5.89506\n",
            "Epoch: 0284 train_loss= 1.96745 train_acc= 0.43881 val_loss= 1.67865 val_acc= 0.60671 time= 5.87716\n",
            "Epoch: 0285 train_loss= 1.97395 train_acc= 0.43871 val_loss= 1.67775 val_acc= 0.60357 time= 5.86508\n",
            "Epoch: 0286 train_loss= 1.97454 train_acc= 0.43986 val_loss= 1.67657 val_acc= 0.60371 time= 5.94376\n",
            "Epoch: 0287 train_loss= 1.97406 train_acc= 0.43752 val_loss= 1.67651 val_acc= 0.60029 time= 5.78356\n",
            "Epoch: 0288 train_loss= 1.96944 train_acc= 0.44157 val_loss= 1.67422 val_acc= 0.60771 time= 5.85041\n",
            "Epoch: 0289 train_loss= 1.97115 train_acc= 0.43910 val_loss= 1.67218 val_acc= 0.61329 time= 5.77234\n",
            "Epoch: 0290 train_loss= 1.96522 train_acc= 0.44371 val_loss= 1.67236 val_acc= 0.60800 time= 5.77376\n",
            "Epoch: 0291 train_loss= 1.97496 train_acc= 0.44205 val_loss= 1.67291 val_acc= 0.60257 time= 5.79454\n",
            "Epoch: 0292 train_loss= 1.97063 train_acc= 0.43910 val_loss= 1.67007 val_acc= 0.60686 time= 5.69571\n",
            "Epoch: 0293 train_loss= 1.96689 train_acc= 0.44119 val_loss= 1.66790 val_acc= 0.60786 time= 5.72218\n",
            "Epoch: 0294 train_loss= 1.97666 train_acc= 0.43771 val_loss= 1.66662 val_acc= 0.61143 time= 5.73976\n",
            "Epoch: 0295 train_loss= 1.96692 train_acc= 0.44048 val_loss= 1.66688 val_acc= 0.61057 time= 5.70797\n",
            "Epoch: 0296 train_loss= 1.97436 train_acc= 0.43843 val_loss= 1.66507 val_acc= 0.61414 time= 5.65256\n",
            "Epoch: 0297 train_loss= 1.95626 train_acc= 0.44324 val_loss= 1.66348 val_acc= 0.61500 time= 5.70440\n",
            "Epoch: 0298 train_loss= 1.96390 train_acc= 0.44500 val_loss= 1.66295 val_acc= 0.61200 time= 5.74232\n",
            "Epoch: 0299 train_loss= 1.96151 train_acc= 0.44176 val_loss= 1.66346 val_acc= 0.60529 time= 5.71013\n",
            "Epoch: 0300 train_loss= 1.96898 train_acc= 0.44181 val_loss= 1.66008 val_acc= 0.61129 time= 5.67892\n",
            "Epoch: 0301 train_loss= 1.96936 train_acc= 0.44338 val_loss= 1.66025 val_acc= 0.61371 time= 5.67419\n",
            "Epoch: 0302 train_loss= 1.96266 train_acc= 0.44252 val_loss= 1.65947 val_acc= 0.61429 time= 5.70138\n",
            "Epoch: 0303 train_loss= 1.95615 train_acc= 0.44829 val_loss= 1.65770 val_acc= 0.61571 time= 5.67940\n",
            "Epoch: 0304 train_loss= 1.95813 train_acc= 0.44495 val_loss= 1.65695 val_acc= 0.61600 time= 5.66633\n",
            "Epoch: 0305 train_loss= 1.96094 train_acc= 0.44552 val_loss= 1.65680 val_acc= 0.61186 time= 5.63825\n",
            "Epoch: 0306 train_loss= 1.95479 train_acc= 0.44557 val_loss= 1.65494 val_acc= 0.61571 time= 5.65851\n",
            "Epoch: 0307 train_loss= 1.96594 train_acc= 0.44233 val_loss= 1.65483 val_acc= 0.61343 time= 5.69766\n",
            "Epoch: 0308 train_loss= 1.95457 train_acc= 0.44462 val_loss= 1.65357 val_acc= 0.61500 time= 5.64700\n",
            "Epoch: 0309 train_loss= 1.94776 train_acc= 0.44552 val_loss= 1.65161 val_acc= 0.61686 time= 5.70800\n",
            "Epoch: 0310 train_loss= 1.94601 train_acc= 0.44657 val_loss= 1.65121 val_acc= 0.61371 time= 5.64793\n",
            "Epoch: 0311 train_loss= 1.95330 train_acc= 0.44638 val_loss= 1.65066 val_acc= 0.61143 time= 5.64073\n",
            "Epoch: 0312 train_loss= 1.95103 train_acc= 0.44795 val_loss= 1.64828 val_acc= 0.61543 time= 5.72217\n",
            "Epoch: 0313 train_loss= 1.94681 train_acc= 0.44700 val_loss= 1.64614 val_acc= 0.61929 time= 6.06296\n",
            "Epoch: 0314 train_loss= 1.95813 train_acc= 0.44295 val_loss= 1.64690 val_acc= 0.61586 time= 5.65861\n",
            "Epoch: 0315 train_loss= 1.95943 train_acc= 0.44191 val_loss= 1.64638 val_acc= 0.61314 time= 5.63456\n",
            "Epoch: 0316 train_loss= 1.96084 train_acc= 0.44114 val_loss= 1.64405 val_acc= 0.61557 time= 5.66959\n",
            "Epoch: 0317 train_loss= 1.95515 train_acc= 0.44429 val_loss= 1.64282 val_acc= 0.61543 time= 5.63042\n",
            "Epoch: 0318 train_loss= 1.95373 train_acc= 0.44357 val_loss= 1.64138 val_acc= 0.61643 time= 5.68466\n",
            "Epoch: 0319 train_loss= 1.94379 train_acc= 0.45091 val_loss= 1.64038 val_acc= 0.61786 time= 5.69542\n",
            "Epoch: 0320 train_loss= 1.95359 train_acc= 0.44529 val_loss= 1.63997 val_acc= 0.61714 time= 5.69189\n",
            "Epoch: 0321 train_loss= 1.94781 train_acc= 0.44352 val_loss= 1.63848 val_acc= 0.61829 time= 5.63415\n",
            "Epoch: 0322 train_loss= 1.94718 train_acc= 0.44781 val_loss= 1.63693 val_acc= 0.61900 time= 5.61379\n",
            "Epoch: 0323 train_loss= 1.94859 train_acc= 0.45052 val_loss= 1.63735 val_acc= 0.61557 time= 5.72128\n",
            "Epoch: 0324 train_loss= 1.93193 train_acc= 0.45100 val_loss= 1.63648 val_acc= 0.61386 time= 5.62054\n",
            "Epoch: 0325 train_loss= 1.94673 train_acc= 0.44891 val_loss= 1.63543 val_acc= 0.61429 time= 5.61515\n",
            "Epoch: 0326 train_loss= 1.95043 train_acc= 0.44652 val_loss= 1.63303 val_acc= 0.62043 time= 5.64517\n",
            "Epoch: 0327 train_loss= 1.94794 train_acc= 0.44576 val_loss= 1.63350 val_acc= 0.62114 time= 5.58532\n",
            "Epoch: 0328 train_loss= 1.94693 train_acc= 0.44862 val_loss= 1.63428 val_acc= 0.61943 time= 5.60953\n",
            "Epoch: 0329 train_loss= 1.93525 train_acc= 0.45191 val_loss= 1.63306 val_acc= 0.61800 time= 5.58600\n",
            "Epoch: 0330 train_loss= 1.94834 train_acc= 0.44843 val_loss= 1.63204 val_acc= 0.61686 time= 5.63842\n",
            "Epoch: 0331 train_loss= 1.95122 train_acc= 0.44748 val_loss= 1.63171 val_acc= 0.61457 time= 5.63214\n",
            "Epoch: 0332 train_loss= 1.94657 train_acc= 0.44786 val_loss= 1.63030 val_acc= 0.61657 time= 5.60995\n",
            "Epoch: 0333 train_loss= 1.94168 train_acc= 0.44814 val_loss= 1.62847 val_acc= 0.61986 time= 5.63613\n",
            "Epoch: 0334 train_loss= 1.94831 train_acc= 0.45067 val_loss= 1.62776 val_acc= 0.62143 time= 5.70438\n",
            "Epoch: 0335 train_loss= 1.93690 train_acc= 0.44938 val_loss= 1.62696 val_acc= 0.62300 time= 5.64872\n",
            "Epoch: 0336 train_loss= 1.94074 train_acc= 0.44724 val_loss= 1.62597 val_acc= 0.62257 time= 5.64826\n",
            "Epoch: 0337 train_loss= 1.93631 train_acc= 0.45414 val_loss= 1.62777 val_acc= 0.61671 time= 5.62531\n",
            "Epoch: 0338 train_loss= 1.94274 train_acc= 0.44891 val_loss= 1.62650 val_acc= 0.61643 time= 5.61284\n",
            "Epoch: 0339 train_loss= 1.94943 train_acc= 0.44786 val_loss= 1.62484 val_acc= 0.62100 time= 5.69433\n",
            "Epoch: 0340 train_loss= 1.94574 train_acc= 0.45095 val_loss= 1.62483 val_acc= 0.62314 time= 5.60351\n",
            "Epoch: 0341 train_loss= 1.94243 train_acc= 0.45048 val_loss= 1.62642 val_acc= 0.62171 time= 5.67769\n",
            "Epoch: 0342 train_loss= 1.93895 train_acc= 0.45348 val_loss= 1.62389 val_acc= 0.62529 time= 5.66091\n",
            "Epoch: 0343 train_loss= 1.94134 train_acc= 0.44714 val_loss= 1.62354 val_acc= 0.62457 time= 5.64137\n",
            "Epoch: 0344 train_loss= 1.94922 train_acc= 0.45195 val_loss= 1.62471 val_acc= 0.62357 time= 5.70305\n",
            "Epoch: 0345 train_loss= 1.95239 train_acc= 0.44500 val_loss= 1.62708 val_acc= 0.61571 time= 5.68250\n",
            "Epoch: 0346 train_loss= 1.93325 train_acc= 0.45024 val_loss= 1.62553 val_acc= 0.61729 time= 5.67718\n",
            "Epoch: 0347 train_loss= 1.92817 train_acc= 0.45557 val_loss= 1.62037 val_acc= 0.62614 time= 5.71571\n",
            "Epoch: 0348 train_loss= 1.94622 train_acc= 0.44833 val_loss= 1.61959 val_acc= 0.62657 time= 5.67027\n",
            "Epoch: 0349 train_loss= 1.92721 train_acc= 0.45429 val_loss= 1.62048 val_acc= 0.62057 time= 5.67982\n",
            "Epoch: 0350 train_loss= 1.93252 train_acc= 0.45157 val_loss= 1.61789 val_acc= 0.62300 time= 5.69497\n",
            "Epoch: 0351 train_loss= 1.93808 train_acc= 0.45314 val_loss= 1.61367 val_acc= 0.62900 time= 5.69337\n",
            "Epoch: 0352 train_loss= 1.93560 train_acc= 0.44991 val_loss= 1.61160 val_acc= 0.62814 time= 5.67820\n",
            "Epoch: 0353 train_loss= 1.91911 train_acc= 0.45967 val_loss= 1.61382 val_acc= 0.61929 time= 5.68994\n",
            "Epoch: 0354 train_loss= 1.93163 train_acc= 0.45100 val_loss= 1.61358 val_acc= 0.62043 time= 5.66038\n",
            "Epoch: 0355 train_loss= 1.92770 train_acc= 0.45333 val_loss= 1.60876 val_acc= 0.63114 time= 5.72191\n",
            "Epoch: 0356 train_loss= 1.92485 train_acc= 0.45491 val_loss= 1.60823 val_acc= 0.63257 time= 5.91019\n",
            "Epoch: 0357 train_loss= 1.92611 train_acc= 0.45571 val_loss= 1.60994 val_acc= 0.62771 time= 5.68437\n",
            "Epoch: 0358 train_loss= 1.92388 train_acc= 0.45519 val_loss= 1.60858 val_acc= 0.62657 time= 5.61252\n",
            "Epoch: 0359 train_loss= 1.92001 train_acc= 0.45638 val_loss= 1.60603 val_acc= 0.62957 time= 5.61809\n",
            "Epoch: 0360 train_loss= 1.94066 train_acc= 0.45238 val_loss= 1.60607 val_acc= 0.62886 time= 5.65093\n",
            "Epoch: 0361 train_loss= 1.92352 train_acc= 0.45505 val_loss= 1.60724 val_acc= 0.62757 time= 5.59117\n",
            "Epoch: 0362 train_loss= 1.91963 train_acc= 0.45443 val_loss= 1.60552 val_acc= 0.63343 time= 5.62285\n",
            "Epoch: 0363 train_loss= 1.93294 train_acc= 0.45476 val_loss= 1.60500 val_acc= 0.63329 time= 5.62058\n",
            "Epoch: 0364 train_loss= 1.93359 train_acc= 0.45195 val_loss= 1.60518 val_acc= 0.62943 time= 5.64015\n",
            "Epoch: 0365 train_loss= 1.91368 train_acc= 0.45667 val_loss= 1.60355 val_acc= 0.62786 time= 5.67766\n",
            "Epoch: 0366 train_loss= 1.92705 train_acc= 0.45733 val_loss= 1.60300 val_acc= 0.62486 time= 5.61657\n",
            "Epoch: 0367 train_loss= 1.92637 train_acc= 0.45352 val_loss= 1.60198 val_acc= 0.62571 time= 5.70683\n",
            "Epoch: 0368 train_loss= 1.93104 train_acc= 0.45338 val_loss= 1.60165 val_acc= 0.62586 time= 5.75047\n",
            "Epoch: 0369 train_loss= 1.91748 train_acc= 0.46062 val_loss= 1.59815 val_acc= 0.63071 time= 5.69905\n",
            "Epoch: 0370 train_loss= 1.92728 train_acc= 0.45367 val_loss= 1.59739 val_acc= 0.63214 time= 5.65800\n",
            "Epoch: 0371 train_loss= 1.90910 train_acc= 0.46257 val_loss= 1.59872 val_acc= 0.62786 time= 5.66819\n",
            "Epoch: 0372 train_loss= 1.92784 train_acc= 0.45824 val_loss= 1.60036 val_acc= 0.62429 time= 5.66178\n",
            "Epoch: 0373 train_loss= 1.91768 train_acc= 0.45800 val_loss= 1.59897 val_acc= 0.62843 time= 5.64274\n",
            "Epoch: 0374 train_loss= 1.91902 train_acc= 0.45791 val_loss= 1.59961 val_acc= 0.62829 time= 5.63474\n",
            "Epoch: 0375 train_loss= 1.92494 train_acc= 0.45952 val_loss= 1.59961 val_acc= 0.62914 time= 5.59978\n",
            "Epoch: 0376 train_loss= 1.91211 train_acc= 0.45933 val_loss= 1.59738 val_acc= 0.63086 time= 5.76271\n",
            "Epoch: 0377 train_loss= 1.92449 train_acc= 0.45419 val_loss= 1.59522 val_acc= 0.63314 time= 5.61053\n",
            "Epoch: 0378 train_loss= 1.92699 train_acc= 0.45852 val_loss= 1.59487 val_acc= 0.62900 time= 5.67149\n",
            "Epoch: 0379 train_loss= 1.91541 train_acc= 0.45862 val_loss= 1.59361 val_acc= 0.62714 time= 5.62865\n",
            "Epoch: 0380 train_loss= 1.91741 train_acc= 0.45643 val_loss= 1.59078 val_acc= 0.63214 time= 5.64346\n",
            "Epoch: 0381 train_loss= 1.91020 train_acc= 0.46257 val_loss= 1.58915 val_acc= 0.63386 time= 5.63061\n",
            "Epoch: 0382 train_loss= 1.91615 train_acc= 0.45695 val_loss= 1.59075 val_acc= 0.63186 time= 5.57752\n",
            "Epoch: 0383 train_loss= 1.91952 train_acc= 0.45991 val_loss= 1.59083 val_acc= 0.63200 time= 5.60229\n",
            "Epoch: 0384 train_loss= 1.91062 train_acc= 0.45705 val_loss= 1.58878 val_acc= 0.63343 time= 5.57785\n",
            "Epoch: 0385 train_loss= 1.92083 train_acc= 0.45881 val_loss= 1.58920 val_acc= 0.63214 time= 5.59881\n",
            "Epoch: 0386 train_loss= 1.90590 train_acc= 0.46191 val_loss= 1.59044 val_acc= 0.62957 time= 5.58224\n",
            "Epoch: 0387 train_loss= 1.90564 train_acc= 0.45748 val_loss= 1.59201 val_acc= 0.62857 time= 5.63332\n",
            "Epoch: 0388 train_loss= 1.90426 train_acc= 0.46310 val_loss= 1.58816 val_acc= 0.63657 time= 5.60448\n",
            "Epoch: 0389 train_loss= 1.91255 train_acc= 0.45995 val_loss= 1.58496 val_acc= 0.64171 time= 5.67055\n",
            "Epoch: 0390 train_loss= 1.91294 train_acc= 0.45748 val_loss= 1.58407 val_acc= 0.63843 time= 5.62256\n",
            "Epoch: 0391 train_loss= 1.91172 train_acc= 0.46048 val_loss= 1.58325 val_acc= 0.63400 time= 5.62933\n",
            "Epoch: 0392 train_loss= 1.90083 train_acc= 0.46110 val_loss= 1.58117 val_acc= 0.63586 time= 5.73514\n",
            "Epoch: 0393 train_loss= 1.90117 train_acc= 0.46305 val_loss= 1.58006 val_acc= 0.63857 time= 5.63581\n",
            "Epoch: 0394 train_loss= 1.91154 train_acc= 0.45900 val_loss= 1.58001 val_acc= 0.63743 time= 5.74005\n",
            "Epoch: 0395 train_loss= 1.91107 train_acc= 0.46010 val_loss= 1.57998 val_acc= 0.63871 time= 5.69297\n",
            "Epoch: 0396 train_loss= 1.91049 train_acc= 0.46191 val_loss= 1.58053 val_acc= 0.63814 time= 5.68067\n",
            "Epoch: 0397 train_loss= 1.91079 train_acc= 0.46000 val_loss= 1.58243 val_acc= 0.63671 time= 5.67827\n",
            "Epoch: 0398 train_loss= 1.91655 train_acc= 0.45471 val_loss= 1.58533 val_acc= 0.63529 time= 5.73603\n",
            "Epoch: 0399 train_loss= 1.90357 train_acc= 0.46210 val_loss= 1.58462 val_acc= 0.63829 time= 5.67411\n",
            "Epoch: 0400 train_loss= 1.90140 train_acc= 0.46262 val_loss= 1.57987 val_acc= 0.64757 time= 5.69462\n",
            "Epoch: 0401 train_loss= 1.90021 train_acc= 0.46557 val_loss= 1.57945 val_acc= 0.64629 time= 5.66155\n",
            "Epoch: 0402 train_loss= 1.89381 train_acc= 0.46648 val_loss= 1.58070 val_acc= 0.63757 time= 5.66481\n",
            "Epoch: 0403 train_loss= 1.90089 train_acc= 0.46071 val_loss= 1.57922 val_acc= 0.63443 time= 5.65387\n",
            "Epoch: 0404 train_loss= 1.89965 train_acc= 0.46229 val_loss= 1.57522 val_acc= 0.63529 time= 5.65043\n",
            "Epoch: 0405 train_loss= 1.90133 train_acc= 0.46271 val_loss= 1.57287 val_acc= 0.63943 time= 5.69573\n",
            "Epoch: 0406 train_loss= 1.90583 train_acc= 0.46414 val_loss= 1.57520 val_acc= 0.63629 time= 5.70467\n",
            "Epoch: 0407 train_loss= 1.90659 train_acc= 0.46038 val_loss= 1.57589 val_acc= 0.63814 time= 5.66353\n",
            "Epoch: 0408 train_loss= 1.89074 train_acc= 0.46505 val_loss= 1.57174 val_acc= 0.64686 time= 5.69367\n",
            "Epoch: 0409 train_loss= 1.90786 train_acc= 0.46205 val_loss= 1.57224 val_acc= 0.64471 time= 5.72092\n",
            "Epoch: 0410 train_loss= 1.89649 train_acc= 0.46705 val_loss= 1.57581 val_acc= 0.63629 time= 5.73411\n",
            "Epoch: 0411 train_loss= 1.89733 train_acc= 0.46252 val_loss= 1.57373 val_acc= 0.63929 time= 5.67640\n",
            "Epoch: 0412 train_loss= 1.88730 train_acc= 0.46805 val_loss= 1.56991 val_acc= 0.64443 time= 5.65455\n",
            "Epoch: 0413 train_loss= 1.89248 train_acc= 0.46671 val_loss= 1.56937 val_acc= 0.64571 time= 5.69093\n",
            "Epoch: 0414 train_loss= 1.89302 train_acc= 0.46748 val_loss= 1.57099 val_acc= 0.64143 time= 5.71459\n",
            "Epoch: 0415 train_loss= 1.90406 train_acc= 0.45814 val_loss= 1.56806 val_acc= 0.64314 time= 5.72845\n",
            "Epoch: 0416 train_loss= 1.90548 train_acc= 0.45962 val_loss= 1.56596 val_acc= 0.64571 time= 5.65488\n",
            "Epoch: 0417 train_loss= 1.90422 train_acc= 0.46714 val_loss= 1.56698 val_acc= 0.64114 time= 5.69609\n",
            "Epoch: 0418 train_loss= 1.88605 train_acc= 0.46886 val_loss= 1.56664 val_acc= 0.64057 time= 5.69497\n",
            "Epoch: 0419 train_loss= 1.88475 train_acc= 0.46767 val_loss= 1.56328 val_acc= 0.64429 time= 5.67469\n",
            "Epoch: 0420 train_loss= 1.90150 train_acc= 0.46495 val_loss= 1.56243 val_acc= 0.64586 time= 5.69413\n",
            "Epoch: 0421 train_loss= 1.90374 train_acc= 0.46162 val_loss= 1.56360 val_acc= 0.64271 time= 5.64405\n",
            "Epoch: 0422 train_loss= 1.88028 train_acc= 0.47138 val_loss= 1.56263 val_acc= 0.64529 time= 5.69977\n",
            "Epoch: 0423 train_loss= 1.89421 train_acc= 0.46305 val_loss= 1.56202 val_acc= 0.64957 time= 5.66989\n",
            "Epoch: 0424 train_loss= 1.89809 train_acc= 0.46414 val_loss= 1.56161 val_acc= 0.65157 time= 5.71130\n",
            "Epoch: 0425 train_loss= 1.88778 train_acc= 0.46571 val_loss= 1.56204 val_acc= 0.64600 time= 5.62664\n",
            "Epoch: 0426 train_loss= 1.88219 train_acc= 0.47048 val_loss= 1.56089 val_acc= 0.64400 time= 5.63163\n",
            "Epoch: 0427 train_loss= 1.88759 train_acc= 0.46624 val_loss= 1.55957 val_acc= 0.64214 time= 5.71927\n",
            "Epoch: 0428 train_loss= 1.89138 train_acc= 0.46562 val_loss= 1.55893 val_acc= 0.64314 time= 5.66115\n",
            "Epoch: 0429 train_loss= 1.88294 train_acc= 0.47114 val_loss= 1.55837 val_acc= 0.64571 time= 5.68641\n",
            "Epoch: 0430 train_loss= 1.88510 train_acc= 0.47219 val_loss= 1.55813 val_acc= 0.64657 time= 5.71297\n",
            "Epoch: 0431 train_loss= 1.89123 train_acc= 0.46310 val_loss= 1.55815 val_acc= 0.64686 time= 5.71504\n",
            "Epoch: 0432 train_loss= 1.88941 train_acc= 0.47057 val_loss= 1.55915 val_acc= 0.64571 time= 5.69766\n",
            "Epoch: 0433 train_loss= 1.88728 train_acc= 0.47205 val_loss= 1.55981 val_acc= 0.64514 time= 5.65347\n",
            "Epoch: 0434 train_loss= 1.88911 train_acc= 0.46695 val_loss= 1.56016 val_acc= 0.64657 time= 5.71066\n",
            "Epoch: 0435 train_loss= 1.89179 train_acc= 0.46876 val_loss= 1.55713 val_acc= 0.65329 time= 5.64478\n",
            "Epoch: 0436 train_loss= 1.88276 train_acc= 0.47148 val_loss= 1.55391 val_acc= 0.65486 time= 5.71575\n",
            "Epoch: 0437 train_loss= 1.88150 train_acc= 0.47314 val_loss= 1.55328 val_acc= 0.65143 time= 5.64357\n",
            "Epoch: 0438 train_loss= 1.88785 train_acc= 0.46671 val_loss= 1.55461 val_acc= 0.64143 time= 5.62318\n",
            "Epoch: 0439 train_loss= 1.88931 train_acc= 0.46743 val_loss= 1.55397 val_acc= 0.63914 time= 5.66858\n",
            "Epoch: 0440 train_loss= 1.87769 train_acc= 0.47029 val_loss= 1.55088 val_acc= 0.64571 time= 5.66103\n",
            "Epoch: 0441 train_loss= 1.88351 train_acc= 0.47152 val_loss= 1.54954 val_acc= 0.65100 time= 6.31799\n",
            "Epoch: 0442 train_loss= 1.88185 train_acc= 0.47291 val_loss= 1.54995 val_acc= 0.65129 time= 5.62257\n",
            "Epoch: 0443 train_loss= 1.88787 train_acc= 0.47276 val_loss= 1.55176 val_acc= 0.64729 time= 5.66869\n",
            "Epoch: 0444 train_loss= 1.88702 train_acc= 0.46733 val_loss= 1.55275 val_acc= 0.64400 time= 5.62649\n",
            "Epoch: 0445 train_loss= 1.88320 train_acc= 0.47010 val_loss= 1.55295 val_acc= 0.64400 time= 5.62853\n",
            "Epoch: 0446 train_loss= 1.87928 train_acc= 0.46833 val_loss= 1.55087 val_acc= 0.64800 time= 5.70317\n",
            "Epoch: 0447 train_loss= 1.87537 train_acc= 0.47276 val_loss= 1.54821 val_acc= 0.65257 time= 5.67913\n",
            "Epoch: 0448 train_loss= 1.88282 train_acc= 0.47329 val_loss= 1.54694 val_acc= 0.65386 time= 5.57094\n",
            "Epoch: 0449 train_loss= 1.86835 train_acc= 0.47433 val_loss= 1.54551 val_acc= 0.65286 time= 5.60281\n",
            "Epoch: 0450 train_loss= 1.88057 train_acc= 0.47181 val_loss= 1.54601 val_acc= 0.64657 time= 5.65737\n",
            "Epoch: 0451 train_loss= 1.87432 train_acc= 0.47200 val_loss= 1.54458 val_acc= 0.64500 time= 5.62539\n",
            "Epoch: 0452 train_loss= 1.88739 train_acc= 0.47014 val_loss= 1.54406 val_acc= 0.64543 time= 5.61059\n",
            "Epoch: 0453 train_loss= 1.87059 train_acc= 0.47310 val_loss= 1.54362 val_acc= 0.64829 time= 5.61619\n",
            "Epoch: 0454 train_loss= 1.88808 train_acc= 0.46967 val_loss= 1.54497 val_acc= 0.64843 time= 5.67549\n",
            "Epoch: 0455 train_loss= 1.86414 train_acc= 0.47871 val_loss= 1.54346 val_acc= 0.65414 time= 5.66450\n",
            "Epoch: 0456 train_loss= 1.88006 train_acc= 0.46967 val_loss= 1.54399 val_acc= 0.65157 time= 5.62362\n",
            "Epoch: 0457 train_loss= 1.86734 train_acc= 0.47824 val_loss= 1.54389 val_acc= 0.64886 time= 5.64218\n",
            "Epoch: 0458 train_loss= 1.87980 train_acc= 0.47462 val_loss= 1.54530 val_acc= 0.64657 time= 5.59981\n",
            "Epoch: 0459 train_loss= 1.85220 train_acc= 0.47810 val_loss= 1.54250 val_acc= 0.65157 time= 5.62743\n",
            "Epoch: 0460 train_loss= 1.87924 train_acc= 0.47248 val_loss= 1.54117 val_acc= 0.65500 time= 5.60027\n",
            "Epoch: 0461 train_loss= 1.88891 train_acc= 0.47133 val_loss= 1.54175 val_acc= 0.65600 time= 5.62020\n",
            "Epoch: 0462 train_loss= 1.87541 train_acc= 0.47291 val_loss= 1.54123 val_acc= 0.65586 time= 5.62281\n",
            "Epoch: 0463 train_loss= 1.86674 train_acc= 0.47567 val_loss= 1.53936 val_acc= 0.65286 time= 5.61196\n",
            "Epoch: 0464 train_loss= 1.86323 train_acc= 0.47867 val_loss= 1.53767 val_acc= 0.65343 time= 5.59960\n",
            "Epoch: 0465 train_loss= 1.87234 train_acc= 0.47319 val_loss= 1.53565 val_acc= 0.65529 time= 5.56842\n",
            "Epoch: 0466 train_loss= 1.87565 train_acc= 0.47348 val_loss= 1.53410 val_acc= 0.65629 time= 5.68917\n",
            "Epoch: 0467 train_loss= 1.86766 train_acc= 0.47676 val_loss= 1.53440 val_acc= 0.65443 time= 5.60013\n",
            "Epoch: 0468 train_loss= 1.86482 train_acc= 0.47491 val_loss= 1.53654 val_acc= 0.64886 time= 5.62476\n",
            "Epoch: 0469 train_loss= 1.87072 train_acc= 0.47105 val_loss= 1.53576 val_acc= 0.64786 time= 5.62075\n",
            "Epoch: 0470 train_loss= 1.87128 train_acc= 0.47376 val_loss= 1.53452 val_acc= 0.65443 time= 5.58948\n",
            "Epoch: 0471 train_loss= 1.86111 train_acc= 0.47848 val_loss= 1.53340 val_acc= 0.65971 time= 5.68039\n",
            "Epoch: 0472 train_loss= 1.86860 train_acc= 0.47591 val_loss= 1.53286 val_acc= 0.66100 time= 5.62790\n",
            "Epoch: 0473 train_loss= 1.86226 train_acc= 0.47738 val_loss= 1.53232 val_acc= 0.65971 time= 5.64083\n",
            "Epoch: 0474 train_loss= 1.85736 train_acc= 0.47495 val_loss= 1.53208 val_acc= 0.65443 time= 5.67243\n",
            "Epoch: 0475 train_loss= 1.86600 train_acc= 0.47267 val_loss= 1.53167 val_acc= 0.65200 time= 5.64817\n",
            "Epoch: 0476 train_loss= 1.85827 train_acc= 0.47971 val_loss= 1.52997 val_acc= 0.65771 time= 5.74335\n",
            "Epoch: 0477 train_loss= 1.85973 train_acc= 0.48010 val_loss= 1.52915 val_acc= 0.66286 time= 5.72270\n",
            "Epoch: 0478 train_loss= 1.87725 train_acc= 0.47424 val_loss= 1.53070 val_acc= 0.65743 time= 5.70305\n",
            "Epoch: 0479 train_loss= 1.87458 train_acc= 0.47391 val_loss= 1.53147 val_acc= 0.65343 time= 5.64158\n",
            "Epoch: 0480 train_loss= 1.87146 train_acc= 0.47476 val_loss= 1.53135 val_acc= 0.65143 time= 5.66838\n",
            "Epoch: 0481 train_loss= 1.87277 train_acc= 0.47810 val_loss= 1.52994 val_acc= 0.65429 time= 5.66586\n",
            "Epoch: 0482 train_loss= 1.85871 train_acc= 0.47833 val_loss= 1.52780 val_acc= 0.66029 time= 5.79113\n",
            "Epoch: 0483 train_loss= 1.85105 train_acc= 0.48129 val_loss= 1.52636 val_acc= 0.66400 time= 6.33022\n",
            "Epoch: 0484 train_loss= 1.85818 train_acc= 0.48124 val_loss= 1.52649 val_acc= 0.65943 time= 5.76306\n",
            "Epoch: 0485 train_loss= 1.85199 train_acc= 0.48005 val_loss= 1.52520 val_acc= 0.65657 time= 5.68659\n",
            "Epoch: 0486 train_loss= 1.85849 train_acc= 0.47962 val_loss= 1.52437 val_acc= 0.65829 time= 5.66091\n",
            "Epoch: 0487 train_loss= 1.85497 train_acc= 0.47819 val_loss= 1.52368 val_acc= 0.66171 time= 5.71596\n",
            "Epoch: 0488 train_loss= 1.85722 train_acc= 0.47600 val_loss= 1.52286 val_acc= 0.66600 time= 5.64982\n",
            "Epoch: 0489 train_loss= 1.86587 train_acc= 0.47857 val_loss= 1.52254 val_acc= 0.66500 time= 5.75184\n",
            "Epoch: 0490 train_loss= 1.85251 train_acc= 0.48329 val_loss= 1.52122 val_acc= 0.66329 time= 5.72565\n",
            "Epoch: 0491 train_loss= 1.85480 train_acc= 0.48005 val_loss= 1.52011 val_acc= 0.66057 time= 5.68269\n",
            "Epoch: 0492 train_loss= 1.86209 train_acc= 0.47700 val_loss= 1.52000 val_acc= 0.65900 time= 5.76272\n",
            "Epoch: 0493 train_loss= 1.84774 train_acc= 0.48471 val_loss= 1.51903 val_acc= 0.66143 time= 5.64726\n",
            "Epoch: 0494 train_loss= 1.84760 train_acc= 0.48395 val_loss= 1.51843 val_acc= 0.66529 time= 5.71036\n",
            "Epoch: 0495 train_loss= 1.85371 train_acc= 0.47814 val_loss= 1.51834 val_acc= 0.66557 time= 5.67286\n",
            "Epoch: 0496 train_loss= 1.84898 train_acc= 0.48414 val_loss= 1.51965 val_acc= 0.66157 time= 5.63997\n",
            "Epoch: 0497 train_loss= 1.84864 train_acc= 0.48129 val_loss= 1.52012 val_acc= 0.65743 time= 5.69082\n",
            "Epoch: 0498 train_loss= 1.85039 train_acc= 0.48148 val_loss= 1.51852 val_acc= 0.66100 time= 5.61300\n",
            "Epoch: 0499 train_loss= 1.85322 train_acc= 0.48086 val_loss= 1.51838 val_acc= 0.66329 time= 5.70847\n",
            "Epoch: 0500 train_loss= 1.84992 train_acc= 0.48443 val_loss= 1.51882 val_acc= 0.66771 time= 5.67921\n",
            "Epoch: 0501 train_loss= 1.84464 train_acc= 0.48257 val_loss= 1.51916 val_acc= 0.66900 time= 5.62048\n",
            "Epoch: 0502 train_loss= 1.85034 train_acc= 0.47952 val_loss= 1.51703 val_acc= 0.67100 time= 5.64820\n",
            "Epoch: 0503 train_loss= 1.83986 train_acc= 0.48705 val_loss= 1.51618 val_acc= 0.66286 time= 5.75036\n",
            "Epoch: 0504 train_loss= 1.84925 train_acc= 0.48062 val_loss= 1.51703 val_acc= 0.65414 time= 5.71778\n",
            "Epoch: 0505 train_loss= 1.84783 train_acc= 0.48305 val_loss= 1.51322 val_acc= 0.66157 time= 5.79625\n",
            "Epoch: 0506 train_loss= 1.84746 train_acc= 0.48414 val_loss= 1.51022 val_acc= 0.66686 time= 5.67834\n",
            "Epoch: 0507 train_loss= 1.84015 train_acc= 0.48724 val_loss= 1.50995 val_acc= 0.67143 time= 5.67509\n",
            "Epoch: 0508 train_loss= 1.84375 train_acc= 0.48624 val_loss= 1.51197 val_acc= 0.66800 time= 5.70833\n",
            "Epoch: 0509 train_loss= 1.84690 train_acc= 0.48619 val_loss= 1.51434 val_acc= 0.66357 time= 5.66040\n",
            "Epoch: 0510 train_loss= 1.83744 train_acc= 0.48133 val_loss= 1.51638 val_acc= 0.66029 time= 5.67405\n",
            "Epoch: 0511 train_loss= 1.84769 train_acc= 0.48405 val_loss= 1.51664 val_acc= 0.66200 time= 5.69379\n",
            "Epoch: 0512 train_loss= 1.83952 train_acc= 0.48448 val_loss= 1.51568 val_acc= 0.66714 time= 5.66983\n",
            "Epoch: 0513 train_loss= 1.83474 train_acc= 0.48486 val_loss= 1.51254 val_acc= 0.67300 time= 5.76450\n",
            "Epoch: 0514 train_loss= 1.83614 train_acc= 0.48919 val_loss= 1.50990 val_acc= 0.67600 time= 5.70227\n",
            "Epoch: 0515 train_loss= 1.84838 train_acc= 0.48391 val_loss= 1.51073 val_acc= 0.67071 time= 5.69832\n",
            "Epoch: 0516 train_loss= 1.83983 train_acc= 0.48619 val_loss= 1.51100 val_acc= 0.66971 time= 5.72140\n",
            "Epoch: 0517 train_loss= 1.83185 train_acc= 0.49062 val_loss= 1.51020 val_acc= 0.66914 time= 5.66680\n",
            "Epoch: 0518 train_loss= 1.83851 train_acc= 0.48857 val_loss= 1.50905 val_acc= 0.66714 time= 5.69293\n",
            "Epoch: 0519 train_loss= 1.84218 train_acc= 0.48695 val_loss= 1.50773 val_acc= 0.66986 time= 5.70987\n",
            "Epoch: 0520 train_loss= 1.84170 train_acc= 0.48800 val_loss= 1.50739 val_acc= 0.67086 time= 5.79450\n",
            "Epoch: 0521 train_loss= 1.83936 train_acc= 0.48776 val_loss= 1.50798 val_acc= 0.66814 time= 5.74212\n",
            "Epoch: 0522 train_loss= 1.83905 train_acc= 0.48629 val_loss= 1.50888 val_acc= 0.66586 time= 5.68667\n",
            "Epoch: 0523 train_loss= 1.85706 train_acc= 0.47933 val_loss= 1.51051 val_acc= 0.66800 time= 5.66993\n",
            "Epoch: 0524 train_loss= 1.83962 train_acc= 0.48567 val_loss= 1.51004 val_acc= 0.67100 time= 5.71999\n",
            "Epoch: 0525 train_loss= 1.83493 train_acc= 0.48871 val_loss= 1.50981 val_acc= 0.67286 time= 5.66908\n",
            "Epoch: 0526 train_loss= 1.82495 train_acc= 0.49552 val_loss= 1.50772 val_acc= 0.67214 time= 5.82042\n",
            "Epoch: 0527 train_loss= 1.83303 train_acc= 0.49357 val_loss= 1.50597 val_acc= 0.66829 time= 5.68889\n",
            "Epoch: 0528 train_loss= 1.84225 train_acc= 0.48757 val_loss= 1.50618 val_acc= 0.66671 time= 5.68047\n",
            "Epoch: 0529 train_loss= 1.83122 train_acc= 0.48895 val_loss= 1.50470 val_acc= 0.66886 time= 5.73743\n",
            "Epoch: 0530 train_loss= 1.83840 train_acc= 0.48857 val_loss= 1.50268 val_acc= 0.67386 time= 5.64535\n",
            "Epoch: 0531 train_loss= 1.82779 train_acc= 0.48800 val_loss= 1.50213 val_acc= 0.67686 time= 5.72563\n",
            "Epoch: 0532 train_loss= 1.83698 train_acc= 0.48795 val_loss= 1.50320 val_acc= 0.67671 time= 5.66766\n",
            "Epoch: 0533 train_loss= 1.83472 train_acc= 0.48738 val_loss= 1.50431 val_acc= 0.66914 time= 5.69606\n",
            "Epoch: 0534 train_loss= 1.83313 train_acc= 0.48729 val_loss= 1.50426 val_acc= 0.66729 time= 5.71510\n",
            "Epoch: 0535 train_loss= 1.83075 train_acc= 0.49000 val_loss= 1.50241 val_acc= 0.67071 time= 5.70160\n",
            "Epoch: 0536 train_loss= 1.83602 train_acc= 0.48652 val_loss= 1.50193 val_acc= 0.67500 time= 5.69654\n",
            "Epoch: 0537 train_loss= 1.82688 train_acc= 0.49210 val_loss= 1.50073 val_acc= 0.67729 time= 5.70323\n",
            "Epoch: 0538 train_loss= 1.83159 train_acc= 0.49024 val_loss= 1.50068 val_acc= 0.67743 time= 5.70365\n",
            "Epoch: 0539 train_loss= 1.84584 train_acc= 0.48595 val_loss= 1.50173 val_acc= 0.67557 time= 5.69295\n",
            "Epoch: 0540 train_loss= 1.83198 train_acc= 0.49105 val_loss= 1.50321 val_acc= 0.66986 time= 5.70098\n",
            "Epoch: 0541 train_loss= 1.83604 train_acc= 0.49110 val_loss= 1.50280 val_acc= 0.66957 time= 5.80427\n",
            "Epoch: 0542 train_loss= 1.83740 train_acc= 0.48695 val_loss= 1.50169 val_acc= 0.67271 time= 5.71909\n",
            "Epoch: 0543 train_loss= 1.83577 train_acc= 0.48910 val_loss= 1.49974 val_acc= 0.67843 time= 5.71907\n",
            "Epoch: 0544 train_loss= 1.83458 train_acc= 0.49095 val_loss= 1.49898 val_acc= 0.67914 time= 5.66093\n",
            "Epoch: 0545 train_loss= 1.83725 train_acc= 0.48943 val_loss= 1.49974 val_acc= 0.67514 time= 5.64229\n",
            "Epoch: 0546 train_loss= 1.83293 train_acc= 0.49600 val_loss= 1.50022 val_acc= 0.67486 time= 5.62497\n",
            "Epoch: 0547 train_loss= 1.82820 train_acc= 0.49133 val_loss= 1.50170 val_acc= 0.67271 time= 5.62956\n",
            "Epoch: 0548 train_loss= 1.82451 train_acc= 0.49205 val_loss= 1.50189 val_acc= 0.67171 time= 5.59583\n",
            "Epoch: 0549 train_loss= 1.83778 train_acc= 0.48805 val_loss= 1.50177 val_acc= 0.67343 time= 5.64883\n",
            "Epoch: 0550 train_loss= 1.82421 train_acc= 0.49343 val_loss= 1.49834 val_acc= 0.67557 time= 5.61265\n",
            "Epoch: 0551 train_loss= 1.84113 train_acc= 0.48476 val_loss= 1.49861 val_acc= 0.67271 time= 5.60366\n",
            "Epoch: 0552 train_loss= 1.82007 train_acc= 0.49024 val_loss= 1.49871 val_acc= 0.67329 time= 5.70285\n",
            "Epoch: 0553 train_loss= 1.81939 train_acc= 0.49452 val_loss= 1.49822 val_acc= 0.67357 time= 5.63707\n",
            "Epoch: 0554 train_loss= 1.82664 train_acc= 0.49105 val_loss= 1.49586 val_acc= 0.67714 time= 5.62524\n",
            "Epoch: 0555 train_loss= 1.83286 train_acc= 0.49000 val_loss= 1.49560 val_acc= 0.67829 time= 5.61636\n",
            "Epoch: 0556 train_loss= 1.81958 train_acc= 0.49386 val_loss= 1.49639 val_acc= 0.67843 time= 5.65962\n",
            "Epoch: 0557 train_loss= 1.81328 train_acc= 0.49733 val_loss= 1.49666 val_acc= 0.67471 time= 5.65051\n",
            "Epoch: 0558 train_loss= 1.81658 train_acc= 0.49519 val_loss= 1.49555 val_acc= 0.67129 time= 5.75804\n",
            "Epoch: 0559 train_loss= 1.82630 train_acc= 0.49367 val_loss= 1.49387 val_acc= 0.67400 time= 5.66642\n",
            "Epoch: 0560 train_loss= 1.82215 train_acc= 0.49238 val_loss= 1.49366 val_acc= 0.67643 time= 5.62008\n",
            "Epoch: 0561 train_loss= 1.81414 train_acc= 0.49919 val_loss= 1.49420 val_acc= 0.67771 time= 5.81586\n",
            "Epoch: 0562 train_loss= 1.81998 train_acc= 0.49510 val_loss= 1.49435 val_acc= 0.68071 time= 5.71288\n",
            "Epoch: 0563 train_loss= 1.81256 train_acc= 0.49324 val_loss= 1.49280 val_acc= 0.67686 time= 5.75126\n",
            "Epoch: 0564 train_loss= 1.81081 train_acc= 0.49857 val_loss= 1.49147 val_acc= 0.67643 time= 5.64921\n",
            "Epoch: 0565 train_loss= 1.82139 train_acc= 0.49476 val_loss= 1.49197 val_acc= 0.67514 time= 5.67432\n",
            "Epoch: 0566 train_loss= 1.81303 train_acc= 0.49614 val_loss= 1.49109 val_acc= 0.67871 time= 5.70789\n",
            "Epoch: 0567 train_loss= 1.81913 train_acc= 0.49538 val_loss= 1.49008 val_acc= 0.68500 time= 5.63623\n",
            "Epoch: 0568 train_loss= 1.81193 train_acc= 0.49771 val_loss= 1.49100 val_acc= 0.68229 time= 5.70858\n",
            "Epoch: 0569 train_loss= 1.82439 train_acc= 0.49319 val_loss= 1.49427 val_acc= 0.67200 time= 5.61362\n",
            "Epoch: 0570 train_loss= 1.81934 train_acc= 0.49214 val_loss= 1.49344 val_acc= 0.67600 time= 5.71276\n",
            "Epoch: 0571 train_loss= 1.82061 train_acc= 0.49357 val_loss= 1.49266 val_acc= 0.67943 time= 5.64338\n",
            "Epoch: 0572 train_loss= 1.81339 train_acc= 0.49624 val_loss= 1.49235 val_acc= 0.68014 time= 5.66696\n",
            "Epoch: 0573 train_loss= 1.81435 train_acc= 0.49767 val_loss= 1.49281 val_acc= 0.67843 time= 5.65430\n",
            "Epoch: 0574 train_loss= 1.81238 train_acc= 0.49652 val_loss= 1.49297 val_acc= 0.67986 time= 5.65432\n",
            "Epoch: 0575 train_loss= 1.82216 train_acc= 0.49291 val_loss= 1.49408 val_acc= 0.67714 time= 5.64087\n",
            "Epoch: 0576 train_loss= 1.80891 train_acc= 0.49771 val_loss= 1.49450 val_acc= 0.67614 time= 5.70280\n",
            "Epoch: 0577 train_loss= 1.81549 train_acc= 0.49357 val_loss= 1.49324 val_acc= 0.67829 time= 5.71415\n",
            "Epoch: 0578 train_loss= 1.82275 train_acc= 0.49433 val_loss= 1.49283 val_acc= 0.68286 time= 5.68683\n",
            "Epoch: 0579 train_loss= 1.81602 train_acc= 0.49805 val_loss= 1.49050 val_acc= 0.68400 time= 5.75139\n",
            "Epoch: 0580 train_loss= 1.79915 train_acc= 0.50214 val_loss= 1.48893 val_acc= 0.68157 time= 5.67934\n",
            "Epoch: 0581 train_loss= 1.81303 train_acc= 0.49638 val_loss= 1.48791 val_acc= 0.67686 time= 5.65162\n",
            "Epoch: 0582 train_loss= 1.80826 train_acc= 0.50043 val_loss= 1.48753 val_acc= 0.67714 time= 5.67607\n",
            "Epoch: 0583 train_loss= 1.81115 train_acc= 0.49295 val_loss= 1.48377 val_acc= 0.68071 time= 5.62604\n",
            "Epoch: 0584 train_loss= 1.80146 train_acc= 0.49857 val_loss= 1.48265 val_acc= 0.68543 time= 5.72335\n",
            "Epoch: 0585 train_loss= 1.80459 train_acc= 0.49814 val_loss= 1.48344 val_acc= 0.68157 time= 5.68286\n",
            "Epoch: 0586 train_loss= 1.80883 train_acc= 0.49876 val_loss= 1.48748 val_acc= 0.67943 time= 5.70899\n",
            "Epoch: 0587 train_loss= 1.81009 train_acc= 0.49857 val_loss= 1.48756 val_acc= 0.68171 time= 5.77398\n",
            "Epoch: 0588 train_loss= 1.81161 train_acc= 0.49729 val_loss= 1.48966 val_acc= 0.67986 time= 5.68072\n",
            "Epoch: 0589 train_loss= 1.81317 train_acc= 0.49448 val_loss= 1.49260 val_acc= 0.67886 time= 5.75024\n",
            "Epoch: 0590 train_loss= 1.80125 train_acc= 0.50052 val_loss= 1.49444 val_acc= 0.67657 time= 5.76186\n",
            "Epoch: 0591 train_loss= 1.79814 train_acc= 0.49938 val_loss= 1.48951 val_acc= 0.67829 time= 5.74583\n",
            "Epoch: 0592 train_loss= 1.80738 train_acc= 0.49852 val_loss= 1.48602 val_acc= 0.68386 time= 5.69682\n",
            "Epoch: 0593 train_loss= 1.79507 train_acc= 0.50510 val_loss= 1.48242 val_acc= 0.68514 time= 5.65450\n",
            "Epoch: 0594 train_loss= 1.81424 train_acc= 0.49605 val_loss= 1.48051 val_acc= 0.68700 time= 5.72735\n",
            "Epoch: 0595 train_loss= 1.79181 train_acc= 0.50452 val_loss= 1.47885 val_acc= 0.68429 time= 5.69737\n",
            "Epoch: 0596 train_loss= 1.80908 train_acc= 0.49957 val_loss= 1.48052 val_acc= 0.68186 time= 5.67612\n",
            "Epoch: 0597 train_loss= 1.79951 train_acc= 0.50091 val_loss= 1.48219 val_acc= 0.68243 time= 5.72002\n",
            "Epoch: 0598 train_loss= 1.79885 train_acc= 0.50114 val_loss= 1.48402 val_acc= 0.68343 time= 5.70749\n",
            "Epoch: 0599 train_loss= 1.81004 train_acc= 0.49514 val_loss= 1.48463 val_acc= 0.68843 time= 5.72070\n",
            "Epoch: 0600 train_loss= 1.80447 train_acc= 0.49810 val_loss= 1.48578 val_acc= 0.68743 time= 5.69730\n",
            "Epoch: 0601 train_loss= 1.80077 train_acc= 0.50105 val_loss= 1.48631 val_acc= 0.68486 time= 5.71836\n",
            "Epoch: 0602 train_loss= 1.79782 train_acc= 0.49714 val_loss= 1.48684 val_acc= 0.68129 time= 5.64321\n",
            "Epoch: 0603 train_loss= 1.79460 train_acc= 0.50067 val_loss= 1.48545 val_acc= 0.68186 time= 5.71943\n",
            "Epoch: 0604 train_loss= 1.80301 train_acc= 0.49848 val_loss= 1.48394 val_acc= 0.68386 time= 5.68355\n",
            "Epoch: 0605 train_loss= 1.79443 train_acc= 0.50543 val_loss= 1.48347 val_acc= 0.68600 time= 5.74690\n",
            "Epoch: 0606 train_loss= 1.80267 train_acc= 0.50071 val_loss= 1.48302 val_acc= 0.68829 time= 5.70180\n",
            "Epoch: 0607 train_loss= 1.80223 train_acc= 0.49971 val_loss= 1.48211 val_acc= 0.69014 time= 5.67434\n",
            "Epoch: 0608 train_loss= 1.80470 train_acc= 0.50286 val_loss= 1.48217 val_acc= 0.68957 time= 5.68453\n",
            "Epoch: 0609 train_loss= 1.80178 train_acc= 0.49919 val_loss= 1.48286 val_acc= 0.68614 time= 5.61174\n",
            "Epoch: 0610 train_loss= 1.80363 train_acc= 0.50110 val_loss= 1.48336 val_acc= 0.68471 time= 5.70809\n",
            "Epoch: 0611 train_loss= 1.80144 train_acc= 0.50133 val_loss= 1.48265 val_acc= 0.68671 time= 5.60945\n",
            "Epoch: 0612 train_loss= 1.78436 train_acc= 0.50676 val_loss= 1.48204 val_acc= 0.69029 time= 5.65559\n",
            "Epoch: 0613 train_loss= 1.79898 train_acc= 0.50005 val_loss= 1.48224 val_acc= 0.68929 time= 5.70981\n",
            "Epoch: 0614 train_loss= 1.79549 train_acc= 0.50271 val_loss= 1.48455 val_acc= 0.68314 time= 5.60560\n",
            "Epoch: 0615 train_loss= 1.79392 train_acc= 0.50319 val_loss= 1.48482 val_acc= 0.68071 time= 5.67852\n",
            "Epoch: 0616 train_loss= 1.80597 train_acc= 0.49819 val_loss= 1.48417 val_acc= 0.68414 time= 5.66139\n",
            "Epoch: 0617 train_loss= 1.79552 train_acc= 0.50143 val_loss= 1.48225 val_acc= 0.68743 time= 5.64911\n",
            "Epoch: 0618 train_loss= 1.81040 train_acc= 0.49681 val_loss= 1.48132 val_acc= 0.68714 time= 5.64733\n",
            "Epoch: 0619 train_loss= 1.79465 train_acc= 0.50167 val_loss= 1.48158 val_acc= 0.68571 time= 5.69539\n",
            "Epoch: 0620 train_loss= 1.78466 train_acc= 0.50419 val_loss= 1.48090 val_acc= 0.68714 time= 5.66917\n",
            "Epoch: 0621 train_loss= 1.78196 train_acc= 0.50438 val_loss= 1.48003 val_acc= 0.68986 time= 5.71649\n",
            "Epoch: 0622 train_loss= 1.78334 train_acc= 0.50681 val_loss= 1.47911 val_acc= 0.69071 time= 5.70552\n",
            "Epoch: 0623 train_loss= 1.78485 train_acc= 0.50543 val_loss= 1.47749 val_acc= 0.69086 time= 5.66791\n",
            "Epoch: 0624 train_loss= 1.79052 train_acc= 0.50286 val_loss= 1.47622 val_acc= 0.68657 time= 5.73130\n",
            "Epoch: 0625 train_loss= 1.78037 train_acc= 0.50871 val_loss= 1.47420 val_acc= 0.68557 time= 5.68512\n",
            "Epoch: 0626 train_loss= 1.79557 train_acc= 0.50129 val_loss= 1.47330 val_acc= 0.68586 time= 5.70612\n",
            "Epoch: 0627 train_loss= 1.78656 train_acc= 0.50586 val_loss= 1.47382 val_acc= 0.68786 time= 5.64096\n",
            "Epoch: 0628 train_loss= 1.78508 train_acc= 0.50571 val_loss= 1.47518 val_acc= 0.68829 time= 5.67578\n",
            "Epoch: 0629 train_loss= 1.78753 train_acc= 0.50671 val_loss= 1.47616 val_acc= 0.68743 time= 5.66480\n",
            "Epoch: 0630 train_loss= 1.79631 train_acc= 0.50271 val_loss= 1.47852 val_acc= 0.68414 time= 5.61476\n",
            "Epoch: 0631 train_loss= 1.78876 train_acc= 0.50638 val_loss= 1.48020 val_acc= 0.68657 time= 5.66480\n",
            "Epoch: 0632 train_loss= 1.78220 train_acc= 0.50505 val_loss= 1.48074 val_acc= 0.68929 time= 5.64073\n",
            "Epoch: 0633 train_loss= 1.79047 train_acc= 0.50110 val_loss= 1.48109 val_acc= 0.69129 time= 5.66214\n",
            "Epoch: 0634 train_loss= 1.77852 train_acc= 0.50724 val_loss= 1.48149 val_acc= 0.68886 time= 5.65128\n",
            "Epoch: 0635 train_loss= 1.78723 train_acc= 0.50705 val_loss= 1.48137 val_acc= 0.68771 time= 5.64391\n",
            "Epoch: 0636 train_loss= 1.79340 train_acc= 0.49933 val_loss= 1.48130 val_acc= 0.68800 time= 5.63064\n",
            "Epoch: 0637 train_loss= 1.78295 train_acc= 0.50700 val_loss= 1.47897 val_acc= 0.69129 time= 5.69535\n",
            "Epoch: 0638 train_loss= 1.78812 train_acc= 0.50419 val_loss= 1.47750 val_acc= 0.68971 time= 5.65890\n",
            "Epoch: 0639 train_loss= 1.77685 train_acc= 0.50748 val_loss= 1.47720 val_acc= 0.68514 time= 5.64023\n",
            "Epoch: 0640 train_loss= 1.77251 train_acc= 0.50910 val_loss= 1.47610 val_acc= 0.68657 time= 5.71281\n",
            "Epoch: 0641 train_loss= 1.79332 train_acc= 0.50081 val_loss= 1.47629 val_acc= 0.68786 time= 5.66216\n",
            "Epoch: 0642 train_loss= 1.77322 train_acc= 0.50948 val_loss= 1.47449 val_acc= 0.69214 time= 5.69620\n",
            "Epoch: 0643 train_loss= 1.80274 train_acc= 0.50191 val_loss= 1.47683 val_acc= 0.69200 time= 5.67541\n",
            "Epoch: 0644 train_loss= 1.79312 train_acc= 0.50362 val_loss= 1.48147 val_acc= 0.69229 time= 5.67081\n",
            "Epoch: 0645 train_loss= 1.78023 train_acc= 0.50662 val_loss= 1.48611 val_acc= 0.68529 time= 5.66872\n",
            "Epoch: 0646 train_loss= 1.77724 train_acc= 0.50614 val_loss= 1.48446 val_acc= 0.68500 time= 5.63830\n",
            "Epoch: 0647 train_loss= 1.77612 train_acc= 0.50557 val_loss= 1.48109 val_acc= 0.68857 time= 5.76788\n",
            "Epoch: 0648 train_loss= 1.78161 train_acc= 0.50629 val_loss= 1.47850 val_acc= 0.69014 time= 5.71412\n",
            "Epoch: 0649 train_loss= 1.77751 train_acc= 0.50505 val_loss= 1.47598 val_acc= 0.69014 time= 5.67351\n",
            "Epoch: 0650 train_loss= 1.79093 train_acc= 0.50219 val_loss= 1.47468 val_acc= 0.69029 time= 5.74282\n",
            "Epoch: 0651 train_loss= 1.78453 train_acc= 0.50510 val_loss= 1.47505 val_acc= 0.69029 time= 5.67428\n",
            "Epoch: 0652 train_loss= 1.78676 train_acc= 0.50443 val_loss= 1.47684 val_acc= 0.69343 time= 5.65787\n",
            "Epoch: 0653 train_loss= 1.76434 train_acc= 0.51605 val_loss= 1.47675 val_acc= 0.69657 time= 5.67663\n",
            "Epoch: 0654 train_loss= 1.78675 train_acc= 0.50114 val_loss= 1.47670 val_acc= 0.69400 time= 5.66623\n",
            "Epoch: 0655 train_loss= 1.77153 train_acc= 0.51043 val_loss= 1.47721 val_acc= 0.68829 time= 5.74739\n",
            "Epoch: 0656 train_loss= 1.75284 train_acc= 0.51281 val_loss= 1.47409 val_acc= 0.69271 time= 5.74811\n",
            "Epoch: 0657 train_loss= 1.75972 train_acc= 0.51371 val_loss= 1.47096 val_acc= 0.69514 time= 5.69269\n",
            "Epoch: 0658 train_loss= 1.78257 train_acc= 0.51129 val_loss= 1.47036 val_acc= 0.69471 time= 5.67730\n",
            "Epoch: 0659 train_loss= 1.77402 train_acc= 0.50452 val_loss= 1.47172 val_acc= 0.69157 time= 5.66790\n",
            "Epoch: 0660 train_loss= 1.75920 train_acc= 0.51086 val_loss= 1.47295 val_acc= 0.69057 time= 5.62692\n",
            "Epoch: 0661 train_loss= 1.77761 train_acc= 0.50924 val_loss= 1.47396 val_acc= 0.69029 time= 5.74811\n",
            "Epoch: 0662 train_loss= 1.76156 train_acc= 0.51462 val_loss= 1.47251 val_acc= 0.69500 time= 5.65132\n",
            "Epoch: 0663 train_loss= 1.77344 train_acc= 0.51000 val_loss= 1.47231 val_acc= 0.69871 time= 5.69137\n",
            "Epoch: 0664 train_loss= 1.76053 train_acc= 0.51552 val_loss= 1.47223 val_acc= 0.69600 time= 5.69947\n",
            "Epoch: 0665 train_loss= 1.77185 train_acc= 0.51314 val_loss= 1.47326 val_acc= 0.69129 time= 5.66780\n",
            "Epoch: 0666 train_loss= 1.75896 train_acc= 0.51786 val_loss= 1.47310 val_acc= 0.68971 time= 5.66901\n",
            "Epoch: 0667 train_loss= 1.77783 train_acc= 0.50452 val_loss= 1.47436 val_acc= 0.69400 time= 5.61556\n",
            "Epoch: 0668 train_loss= 1.76762 train_acc= 0.51205 val_loss= 1.47591 val_acc= 0.69529 time= 5.68902\n",
            "Epoch: 0669 train_loss= 1.75571 train_acc= 0.51124 val_loss= 1.47559 val_acc= 0.69086 time= 5.64902\n",
            "Epoch: 0670 train_loss= 1.77110 train_acc= 0.50662 val_loss= 1.47486 val_acc= 0.68814 time= 5.70309\n",
            "Epoch: 0671 train_loss= 1.76941 train_acc= 0.50895 val_loss= 1.47385 val_acc= 0.69157 time= 5.73827\n",
            "Epoch: 0672 train_loss= 1.77196 train_acc= 0.51171 val_loss= 1.47294 val_acc= 0.69471 time= 5.71466\n",
            "Epoch: 0673 train_loss= 1.77014 train_acc= 0.51205 val_loss= 1.47093 val_acc= 0.69957 time= 5.67159\n",
            "Epoch: 0674 train_loss= 1.76160 train_acc= 0.51433 val_loss= 1.47064 val_acc= 0.69671 time= 5.66344\n",
            "Epoch: 0675 train_loss= 1.76338 train_acc= 0.51276 val_loss= 1.47222 val_acc= 0.68386 time= 5.63584\n",
            "Epoch: 0676 train_loss= 1.78211 train_acc= 0.50257 val_loss= 1.47393 val_acc= 0.68200 time= 5.64331\n",
            "Epoch: 0677 train_loss= 1.75949 train_acc= 0.51157 val_loss= 1.47169 val_acc= 0.69157 time= 5.67081\n",
            "Epoch: 0678 train_loss= 1.76055 train_acc= 0.51400 val_loss= 1.47217 val_acc= 0.69786 time= 5.61000\n",
            "Epoch: 0679 train_loss= 1.77376 train_acc= 0.50867 val_loss= 1.47455 val_acc= 0.69729 time= 5.66055\n",
            "Epoch: 0680 train_loss= 1.75408 train_acc= 0.51767 val_loss= 1.47619 val_acc= 0.69214 time= 5.62545\n",
            "Epoch: 0681 train_loss= 1.75249 train_acc= 0.51548 val_loss= 1.47371 val_acc= 0.69343 time= 5.64096\n",
            "Epoch: 0682 train_loss= 1.75641 train_acc= 0.51329 val_loss= 1.47150 val_acc= 0.69600 time= 5.67426\n",
            "Epoch: 0683 train_loss= 1.76031 train_acc= 0.51795 val_loss= 1.47049 val_acc= 0.69929 time= 5.63045\n",
            "Epoch: 0684 train_loss= 1.74997 train_acc= 0.51505 val_loss= 1.46924 val_acc= 0.69686 time= 5.67123\n",
            "Epoch: 0685 train_loss= 1.75596 train_acc= 0.51443 val_loss= 1.46838 val_acc= 0.69157 time= 5.68440\n",
            "Epoch: 0686 train_loss= 1.75799 train_acc= 0.51519 val_loss= 1.46733 val_acc= 0.69414 time= 5.64653\n",
            "Epoch: 0687 train_loss= 1.75101 train_acc= 0.51510 val_loss= 1.46645 val_acc= 0.70071 time= 5.64886\n",
            "Epoch: 0688 train_loss= 1.77372 train_acc= 0.51095 val_loss= 1.46902 val_acc= 0.69914 time= 5.62214\n",
            "Epoch: 0689 train_loss= 1.76628 train_acc= 0.51205 val_loss= 1.47176 val_acc= 0.69457 time= 5.67509\n",
            "Epoch: 0690 train_loss= 1.75896 train_acc= 0.51271 val_loss= 1.47260 val_acc= 0.69229 time= 5.66826\n",
            "Epoch: 0691 train_loss= 1.75038 train_acc= 0.51552 val_loss= 1.47356 val_acc= 0.69357 time= 5.65933\n",
            "Epoch: 0692 train_loss= 1.74881 train_acc= 0.52024 val_loss= 1.47360 val_acc= 0.69786 time= 5.61925\n",
            "Epoch: 0693 train_loss= 1.75121 train_acc= 0.51729 val_loss= 1.47201 val_acc= 0.69671 time= 5.66849\n",
            "Epoch: 0694 train_loss= 1.74691 train_acc= 0.51800 val_loss= 1.46929 val_acc= 0.69400 time= 5.64960\n",
            "Epoch: 0695 train_loss= 1.76387 train_acc= 0.51267 val_loss= 1.46801 val_acc= 0.69329 time= 5.67965\n",
            "Epoch: 0696 train_loss= 1.75345 train_acc= 0.51548 val_loss= 1.46687 val_acc= 0.69829 time= 5.64207\n",
            "Epoch: 0697 train_loss= 1.75491 train_acc= 0.51643 val_loss= 1.46766 val_acc= 0.69471 time= 5.61737\n",
            "Epoch: 0698 train_loss= 1.75345 train_acc= 0.51662 val_loss= 1.46951 val_acc= 0.69371 time= 5.63043\n",
            "Epoch: 0699 train_loss= 1.76221 train_acc= 0.50991 val_loss= 1.47417 val_acc= 0.68943 time= 5.62603\n",
            "Epoch: 0700 train_loss= 1.75471 train_acc= 0.51633 val_loss= 1.47514 val_acc= 0.69429 time= 5.66657\n",
            "Epoch: 0701 train_loss= 1.74723 train_acc= 0.51562 val_loss= 1.47483 val_acc= 0.69929 time= 5.62265\n",
            "Epoch: 0702 train_loss= 1.76509 train_acc= 0.51562 val_loss= 1.47442 val_acc= 0.69886 time= 5.63451\n",
            "Epoch: 0703 train_loss= 1.74197 train_acc= 0.51995 val_loss= 1.47294 val_acc= 0.69186 time= 5.61834\n",
            "Epoch: 0704 train_loss= 1.75669 train_acc= 0.51567 val_loss= 1.47082 val_acc= 0.68886 time= 5.61381\n",
            "Epoch: 0705 train_loss= 1.75075 train_acc= 0.52038 val_loss= 1.46754 val_acc= 0.69443 time= 5.62753\n",
            "Epoch: 0706 train_loss= 1.73763 train_acc= 0.52100 val_loss= 1.46541 val_acc= 0.69986 time= 5.60008\n",
            "Epoch: 0707 train_loss= 1.74922 train_acc= 0.51586 val_loss= 1.46464 val_acc= 0.70100 time= 5.61082\n",
            "Epoch: 0708 train_loss= 1.75235 train_acc= 0.51471 val_loss= 1.46515 val_acc= 0.69771 time= 5.59888\n",
            "Epoch: 0709 train_loss= 1.74516 train_acc= 0.51757 val_loss= 1.46677 val_acc= 0.69586 time= 5.66290\n",
            "Epoch: 0710 train_loss= 1.74369 train_acc= 0.52048 val_loss= 1.46709 val_acc= 0.70257 time= 5.59912\n",
            "Epoch: 0711 train_loss= 1.73595 train_acc= 0.52238 val_loss= 1.46750 val_acc= 0.70271 time= 5.61552\n",
            "Epoch: 0712 train_loss= 1.73117 train_acc= 0.52362 val_loss= 1.46719 val_acc= 0.69886 time= 5.65166\n",
            "Epoch: 0713 train_loss= 1.74701 train_acc= 0.51867 val_loss= 1.46746 val_acc= 0.69314 time= 5.57447\n",
            "Epoch: 0714 train_loss= 1.74983 train_acc= 0.51862 val_loss= 1.46822 val_acc= 0.69300 time= 5.67323\n",
            "Epoch: 0715 train_loss= 1.75823 train_acc= 0.51581 val_loss= 1.46948 val_acc= 0.70457 time= 5.59543\n",
            "Epoch: 0716 train_loss= 1.74445 train_acc= 0.52252 val_loss= 1.47148 val_acc= 0.70700 time= 5.58796\n",
            "Epoch: 0717 train_loss= 1.74377 train_acc= 0.52114 val_loss= 1.47313 val_acc= 0.70057 time= 5.62115\n",
            "Epoch: 0718 train_loss= 1.74303 train_acc= 0.51681 val_loss= 1.47561 val_acc= 0.68843 time= 5.60190\n",
            "Epoch: 0719 train_loss= 1.73316 train_acc= 0.52138 val_loss= 1.47440 val_acc= 0.69086 time= 5.58077\n",
            "Epoch: 0720 train_loss= 1.73482 train_acc= 0.52105 val_loss= 1.47098 val_acc= 0.70214 time= 5.64042\n",
            "Epoch: 0721 train_loss= 1.74283 train_acc= 0.51762 val_loss= 1.46992 val_acc= 0.70729 time= 5.61435\n",
            "Epoch: 0722 train_loss= 1.73721 train_acc= 0.52238 val_loss= 1.46799 val_acc= 0.70071 time= 5.65315\n",
            "Epoch: 0723 train_loss= 1.73579 train_acc= 0.52652 val_loss= 1.46736 val_acc= 0.69200 time= 5.60565\n",
            "Epoch: 0724 train_loss= 1.73498 train_acc= 0.51891 val_loss= 1.46519 val_acc= 0.69357 time= 5.59757\n",
            "Epoch: 0725 train_loss= 1.74130 train_acc= 0.52076 val_loss= 1.46270 val_acc= 0.70143 time= 5.66590\n",
            "Epoch: 0726 train_loss= 1.73823 train_acc= 0.52162 val_loss= 1.46124 val_acc= 0.70871 time= 5.61014\n",
            "Epoch: 0727 train_loss= 1.73412 train_acc= 0.52281 val_loss= 1.46067 val_acc= 0.70500 time= 5.63998\n",
            "Epoch: 0728 train_loss= 1.73519 train_acc= 0.52567 val_loss= 1.46231 val_acc= 0.69929 time= 5.57662\n",
            "Epoch: 0729 train_loss= 1.74502 train_acc= 0.52195 val_loss= 1.46659 val_acc= 0.69557 time= 5.62146\n",
            "Epoch: 0730 train_loss= 1.73127 train_acc= 0.52291 val_loss= 1.46962 val_acc= 0.69786 time= 5.64367\n",
            "Epoch: 0731 train_loss= 1.73135 train_acc= 0.52243 val_loss= 1.47159 val_acc= 0.70129 time= 5.64591\n",
            "Epoch: 0732 train_loss= 1.74031 train_acc= 0.51767 val_loss= 1.47480 val_acc= 0.69671 time= 5.67344\n",
            "Epoch: 0733 train_loss= 1.73655 train_acc= 0.52391 val_loss= 1.47668 val_acc= 0.69629 time= 5.65676\n",
            "Epoch: 0734 train_loss= 1.72734 train_acc= 0.52529 val_loss= 1.47468 val_acc= 0.69686 time= 5.62510\n",
            "Epoch: 0735 train_loss= 1.73260 train_acc= 0.52129 val_loss= 1.47202 val_acc= 0.69814 time= 5.63741\n",
            "Epoch: 0736 train_loss= 1.73095 train_acc= 0.52224 val_loss= 1.46962 val_acc= 0.69686 time= 5.64764\n",
            "Epoch: 0737 train_loss= 1.74080 train_acc= 0.52167 val_loss= 1.46813 val_acc= 0.69386 time= 5.58839\n",
            "Epoch: 0738 train_loss= 1.74522 train_acc= 0.51962 val_loss= 1.46805 val_acc= 0.69329 time= 5.62807\n",
            "Epoch: 0739 train_loss= 1.73132 train_acc= 0.52176 val_loss= 1.46775 val_acc= 0.69671 time= 5.61016\n",
            "Epoch: 0740 train_loss= 1.73038 train_acc= 0.52410 val_loss= 1.46803 val_acc= 0.70043 time= 5.58812\n",
            "Epoch: 0741 train_loss= 1.73677 train_acc= 0.51624 val_loss= 1.46958 val_acc= 0.70186 time= 5.56821\n",
            "Epoch: 0742 train_loss= 1.73455 train_acc= 0.52419 val_loss= 1.47057 val_acc= 0.70014 time= 5.57049\n",
            "Epoch: 0743 train_loss= 1.72755 train_acc= 0.52867 val_loss= 1.47075 val_acc= 0.70186 time= 5.60964\n",
            "Epoch: 0744 train_loss= 1.73861 train_acc= 0.51952 val_loss= 1.47085 val_acc= 0.70071 time= 5.62337\n",
            "Epoch: 0745 train_loss= 1.71768 train_acc= 0.52495 val_loss= 1.46961 val_acc= 0.70357 time= 5.56912\n",
            "Epoch: 0746 train_loss= 1.73131 train_acc= 0.52257 val_loss= 1.46889 val_acc= 0.70514 time= 5.62645\n",
            "Epoch: 0747 train_loss= 1.73474 train_acc= 0.52295 val_loss= 1.46936 val_acc= 0.70400 time= 5.55913\n",
            "Epoch: 0748 train_loss= 1.72985 train_acc= 0.52557 val_loss= 1.47013 val_acc= 0.69743 time= 5.57818\n",
            "Epoch: 0749 train_loss= 1.72194 train_acc= 0.52652 val_loss= 1.46982 val_acc= 0.69643 time= 5.58960\n",
            "Epoch: 0750 train_loss= 1.72601 train_acc= 0.52429 val_loss= 1.46926 val_acc= 0.69871 time= 5.56481\n",
            "Epoch: 0751 train_loss= 1.72692 train_acc= 0.52400 val_loss= 1.46831 val_acc= 0.70314 time= 5.61175\n",
            "Epoch: 0752 train_loss= 1.73164 train_acc= 0.52491 val_loss= 1.46831 val_acc= 0.70443 time= 5.62562\n",
            "Epoch: 0753 train_loss= 1.72432 train_acc= 0.52367 val_loss= 1.46847 val_acc= 0.70443 time= 5.64307\n",
            "Epoch: 0754 train_loss= 1.73020 train_acc= 0.52138 val_loss= 1.46833 val_acc= 0.70214 time= 5.64369\n",
            "Epoch: 0755 train_loss= 1.72776 train_acc= 0.52124 val_loss= 1.46956 val_acc= 0.70086 time= 5.62111\n",
            "Epoch: 0756 train_loss= 1.73053 train_acc= 0.52410 val_loss= 1.47168 val_acc= 0.69814 time= 5.61578\n",
            "Epoch: 0757 train_loss= 1.72406 train_acc= 0.52748 val_loss= 1.47187 val_acc= 0.69971 time= 5.59055\n",
            "Epoch: 0758 train_loss= 1.72424 train_acc= 0.52743 val_loss= 1.47210 val_acc= 0.70286 time= 5.63694\n",
            "Epoch: 0759 train_loss= 1.72911 train_acc= 0.52657 val_loss= 1.47283 val_acc= 0.70457 time= 5.58660\n",
            "Epoch: 0760 train_loss= 1.71129 train_acc= 0.53143 val_loss= 1.47275 val_acc= 0.70771 time= 5.59764\n",
            "Epoch: 0761 train_loss= 1.73743 train_acc= 0.51995 val_loss= 1.47232 val_acc= 0.70014 time= 5.61443\n",
            "Epoch: 0762 train_loss= 1.71569 train_acc= 0.52876 val_loss= 1.47068 val_acc= 0.69643 time= 5.63913\n",
            "Epoch: 0763 train_loss= 1.72868 train_acc= 0.52405 val_loss= 1.46998 val_acc= 0.69714 time= 5.59742\n",
            "Epoch: 0764 train_loss= 1.72151 train_acc= 0.52667 val_loss= 1.46981 val_acc= 0.70329 time= 5.64579\n",
            "Epoch: 0765 train_loss= 1.71697 train_acc= 0.52786 val_loss= 1.46824 val_acc= 0.70614 time= 5.58788\n",
            "Epoch: 0766 train_loss= 1.71603 train_acc= 0.52919 val_loss= 1.46552 val_acc= 0.70486 time= 5.62588\n",
            "Epoch: 0767 train_loss= 1.71894 train_acc= 0.52981 val_loss= 1.46554 val_acc= 0.69657 time= 5.64344\n",
            "Epoch: 0768 train_loss= 1.73345 train_acc= 0.52319 val_loss= 1.46816 val_acc= 0.69629 time= 5.60046\n",
            "Epoch: 0769 train_loss= 1.72062 train_acc= 0.52414 val_loss= 1.46945 val_acc= 0.70114 time= 5.61871\n",
            "Epoch: 0770 train_loss= 1.71999 train_acc= 0.52557 val_loss= 1.46811 val_acc= 0.70914 time= 5.57420\n",
            "Epoch: 0771 train_loss= 1.72315 train_acc= 0.52381 val_loss= 1.46887 val_acc= 0.70471 time= 5.60039\n",
            "Epoch: 0772 train_loss= 1.71728 train_acc= 0.52852 val_loss= 1.47127 val_acc= 0.69900 time= 5.58610\n",
            "Epoch: 0773 train_loss= 1.71237 train_acc= 0.53014 val_loss= 1.47010 val_acc= 0.70057 time= 5.63796\n",
            "Epoch: 0774 train_loss= 1.71592 train_acc= 0.52638 val_loss= 1.46665 val_acc= 0.70443 time= 5.60752\n",
            "Epoch: 0775 train_loss= 1.72073 train_acc= 0.52857 val_loss= 1.46630 val_acc= 0.70557 time= 5.63104\n",
            "Epoch: 0776 train_loss= 1.71491 train_acc= 0.53171 val_loss= 1.46741 val_acc= 0.70400 time= 5.63403\n",
            "Epoch: 0777 train_loss= 1.71133 train_acc= 0.52814 val_loss= 1.46956 val_acc= 0.69786 time= 5.55890\n",
            "Epoch: 0778 train_loss= 1.71173 train_acc= 0.52714 val_loss= 1.46815 val_acc= 0.69657 time= 5.62580\n",
            "Epoch: 0779 train_loss= 1.71515 train_acc= 0.52871 val_loss= 1.46860 val_acc= 0.70243 time= 5.55235\n",
            "Epoch: 0780 train_loss= 1.71510 train_acc= 0.53019 val_loss= 1.47183 val_acc= 0.70200 time= 5.66416\n",
            "Epoch: 0781 train_loss= 1.70055 train_acc= 0.53267 val_loss= 1.47540 val_acc= 0.69957 time= 5.64661\n",
            "Epoch: 0782 train_loss= 1.70395 train_acc= 0.53257 val_loss= 1.47378 val_acc= 0.69857 time= 5.63561\n",
            "Epoch: 0783 train_loss= 1.70271 train_acc= 0.53191 val_loss= 1.47176 val_acc= 0.69914 time= 5.60073\n",
            "Epoch: 0784 train_loss= 1.70895 train_acc= 0.53195 val_loss= 1.46952 val_acc= 0.70429 time= 5.62636\n",
            "Epoch: 0785 train_loss= 1.70481 train_acc= 0.52862 val_loss= 1.46873 val_acc= 0.70600 time= 5.59816\n",
            "Epoch: 0786 train_loss= 1.71443 train_acc= 0.52776 val_loss= 1.46612 val_acc= 0.70357 time= 5.60330\n",
            "Epoch: 0787 train_loss= 1.70781 train_acc= 0.52767 val_loss= 1.46456 val_acc= 0.69843 time= 5.58782\n",
            "Epoch: 0788 train_loss= 1.71547 train_acc= 0.52800 val_loss= 1.46671 val_acc= 0.69743 time= 5.58780\n",
            "Epoch: 0789 train_loss= 1.70808 train_acc= 0.53319 val_loss= 1.46941 val_acc= 0.70229 time= 5.64481\n",
            "Epoch: 0790 train_loss= 1.71801 train_acc= 0.52419 val_loss= 1.47220 val_acc= 0.70400 time= 5.61281\n",
            "Epoch: 0791 train_loss= 1.69070 train_acc= 0.53862 val_loss= 1.47162 val_acc= 0.70457 time= 5.59558\n",
            "Epoch: 0792 train_loss= 1.71630 train_acc= 0.53086 val_loss= 1.47203 val_acc= 0.70543 time= 5.61626\n",
            "Epoch: 0793 train_loss= 1.70880 train_acc= 0.52976 val_loss= 1.47209 val_acc= 0.70471 time= 5.64431\n",
            "Epoch: 0794 train_loss= 1.69998 train_acc= 0.53300 val_loss= 1.47185 val_acc= 0.70400 time= 5.59882\n",
            "Epoch: 0795 train_loss= 1.71475 train_acc= 0.52710 val_loss= 1.46929 val_acc= 0.70671 time= 5.60454\n",
            "Epoch: 0796 train_loss= 1.70926 train_acc= 0.53029 val_loss= 1.46826 val_acc= 0.70729 time= 5.59591\n",
            "Epoch: 0797 train_loss= 1.70140 train_acc= 0.53648 val_loss= 1.46789 val_acc= 0.70671 time= 5.61106\n",
            "Epoch: 0798 train_loss= 1.70461 train_acc= 0.53052 val_loss= 1.46788 val_acc= 0.70414 time= 5.65081\n",
            "Epoch: 0799 train_loss= 1.71469 train_acc= 0.52619 val_loss= 1.46718 val_acc= 0.70543 time= 5.58958\n",
            "Epoch: 0800 train_loss= 1.70014 train_acc= 0.53319 val_loss= 1.46569 val_acc= 0.70657 time= 5.60519\n",
            "Epoch: 0801 train_loss= 1.71204 train_acc= 0.52891 val_loss= 1.46652 val_acc= 0.70886 time= 5.66202\n",
            "Epoch: 0802 train_loss= 1.70449 train_acc= 0.53591 val_loss= 1.47002 val_acc= 0.70686 time= 5.60440\n",
            "Epoch: 0803 train_loss= 1.70280 train_acc= 0.53286 val_loss= 1.47085 val_acc= 0.70486 time= 5.62093\n",
            "Epoch: 0804 train_loss= 1.70300 train_acc= 0.53381 val_loss= 1.47111 val_acc= 0.70657 time= 5.58445\n",
            "Epoch: 0805 train_loss= 1.70066 train_acc= 0.53152 val_loss= 1.47122 val_acc= 0.70529 time= 5.61240\n",
            "Epoch: 0806 train_loss= 1.69815 train_acc= 0.53567 val_loss= 1.47188 val_acc= 0.70543 time= 5.60236\n",
            "Epoch: 0807 train_loss= 1.69055 train_acc= 0.53805 val_loss= 1.46889 val_acc= 0.70400 time= 5.62946\n",
            "Epoch: 0808 train_loss= 1.69539 train_acc= 0.53819 val_loss= 1.46412 val_acc= 0.70429 time= 5.61736\n",
            "Epoch: 0809 train_loss= 1.70714 train_acc= 0.52938 val_loss= 1.46222 val_acc= 0.70300 time= 5.55606\n",
            "Epoch: 0810 train_loss= 1.70976 train_acc= 0.53371 val_loss= 1.46263 val_acc= 0.70257 time= 5.59231\n",
            "Epoch: 0811 train_loss= 1.69758 train_acc= 0.53157 val_loss= 1.46479 val_acc= 0.70743 time= 5.61782\n",
            "Epoch: 0812 train_loss= 1.70950 train_acc= 0.53533 val_loss= 1.46885 val_acc= 0.70671 time= 5.59536\n",
            "Epoch: 0813 train_loss= 1.68840 train_acc= 0.53833 val_loss= 1.47286 val_acc= 0.70157 time= 5.60262\n",
            "Epoch: 0814 train_loss= 1.69545 train_acc= 0.53576 val_loss= 1.47555 val_acc= 0.69957 time= 5.58500\n",
            "Epoch: 0815 train_loss= 1.68975 train_acc= 0.53491 val_loss= 1.47548 val_acc= 0.70200 time= 5.60109\n",
            "Epoch: 0816 train_loss= 1.69512 train_acc= 0.53252 val_loss= 1.47450 val_acc= 0.70400 time= 5.59926\n",
            "Epoch: 0817 train_loss= 1.69688 train_acc= 0.53529 val_loss= 1.47260 val_acc= 0.70714 time= 5.60779\n",
            "Epoch: 0818 train_loss= 1.68020 train_acc= 0.54143 val_loss= 1.47044 val_acc= 0.70514 time= 5.61353\n",
            "Epoch: 0819 train_loss= 1.69625 train_acc= 0.53286 val_loss= 1.47061 val_acc= 0.70029 time= 5.63316\n",
            "Epoch: 0820 train_loss= 1.68819 train_acc= 0.53724 val_loss= 1.46651 val_acc= 0.70000 time= 5.59739\n",
            "Epoch: 0821 train_loss= 1.70155 train_acc= 0.53233 val_loss= 1.46370 val_acc= 0.70329 time= 5.66507\n",
            "Epoch: 0822 train_loss= 1.69716 train_acc= 0.53324 val_loss= 1.46314 val_acc= 0.71014 time= 5.61401\n",
            "Epoch: 0823 train_loss= 1.68441 train_acc= 0.53933 val_loss= 1.46469 val_acc= 0.70729 time= 5.65260\n",
            "Epoch: 0824 train_loss= 1.69561 train_acc= 0.52991 val_loss= 1.46476 val_acc= 0.70500 time= 5.62843\n",
            "Epoch: 0825 train_loss= 1.68764 train_acc= 0.53919 val_loss= 1.46665 val_acc= 0.70329 time= 5.60698\n",
            "Epoch: 0826 train_loss= 1.68086 train_acc= 0.53700 val_loss= 1.46859 val_acc= 0.70686 time= 5.61174\n",
            "Epoch: 0827 train_loss= 1.69351 train_acc= 0.53348 val_loss= 1.47293 val_acc= 0.70843 time= 5.61880\n",
            "Epoch: 0828 train_loss= 1.69068 train_acc= 0.53752 val_loss= 1.47445 val_acc= 0.70700 time= 5.60449\n",
            "Epoch: 0829 train_loss= 1.69434 train_acc= 0.53410 val_loss= 1.47687 val_acc= 0.70429 time= 5.60318\n",
            "Epoch: 0830 train_loss= 1.69208 train_acc= 0.53895 val_loss= 1.47843 val_acc= 0.70243 time= 5.59303\n",
            "Early stopping...\n",
            "Training completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "BPI5hKwd_xf9",
        "outputId": "8f9713f1-9912-44f8-845a-4a5b8f7e399c"
      },
      "source": [
        "#Visualization of the training progress\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(train_accuracy,label = 'train_accuracy')\n",
        "plt.plot(train_loss,label = 'train_loss')\n",
        "plt.plot(validation_accuracy, label='validation_accuracy')\n",
        "plt.plot(validation_loss,label='validaton_loss')\n",
        "plt.legend(loc=1)\n",
        "axes = plt.gca()\n",
        "plt.savefig('gcn.svg', format='svg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_84Jol-sD_Ai",
        "outputId": "f2e8cb44-6b4c-419b-a574-cdafd6a967a5"
      },
      "source": [
        "#Testing\n",
        "\n",
        "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
        "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
        "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set results: cost= 1.45158 accuracy= 0.71571 time= 1.96241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvj3cqTCJ81"
      },
      "source": [
        "## Observations\n",
        "\n",
        "The dataset is pretty limited and as observed also empirically, even if the structure of the network is extremely simple and this notebook has been developed only for educational purposes with reduced computational capabilities, it is possible to improve the model performances executing another training of the model on another dataset of the same size.\n",
        "\n",
        "*With more computational capabilities available is recomended  to use a bigger dataset since the very beginning.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCvwbzr9AIyG",
        "outputId": "061371bd-16e9-4ab9-b046-daebc012753d"
      },
      "source": [
        "#It is necessary to save the model because to free the memory from the old dataset it is necessary to restart the machine\n",
        "\n",
        "model.save(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved in file: tmp/gcn.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "mosuG8sHEYcs",
        "outputId": "6d188426-bfd0-4256-9a76-9aa20ada8d62"
      },
      "source": [
        "#Download of the weight of the model, useful restore point\n",
        "\n",
        "!zip -r /content/tmp.zip /content/tmp\n",
        "files.download(\"/content/tmp.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/tmp/ (stored 0%)\n",
            "  adding: content/tmp/gcn.ckpt.meta (deflated 89%)\n",
            "  adding: content/tmp/gcn.ckpt.index (deflated 37%)\n",
            "  adding: content/tmp/checkpoint (deflated 38%)\n",
            "  adding: content/tmp/gcn.ckpt.data-00000-of-00001 (deflated 7%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_68285ddc-8263-419c-932b-19c8064551fd\", \"tmp.zip\", 5533719)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e_Qmq7pLDUw"
      },
      "source": [
        "## Prediction pipeline\n",
        "\n",
        "Now that the network is trained it is necessary to structure the prediction functions. in fact, the network is able to predict only the next node in the sequence of the shortest path and not the sequence itself. In order to do it it is necessary to code an iterative evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pesk5QkHvbYl"
      },
      "source": [
        "### Reinitialization\n",
        "\n",
        "It is necessary if the training and the prediction are performed in different moments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkR5pXBWWer"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg5e8SMJMwr5"
      },
      "source": [
        "! mkdir /content/gcn\n",
        "! mv __init__.py gcn\n",
        "! mv inits.py gcn\n",
        "! mv layers.py gcn\n",
        "! mv metrics.py gcn\n",
        "! mv models.py gcn\n",
        "! mv train.py gcn\n",
        "! mv utils.py gcn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4prSruXQQ_OL",
        "outputId": "3f0557e4-6364-49d0-b383-4605605a23cd"
      },
      "source": [
        "!unzip /content/tmp.zip \n",
        "!mv /content/content/tmp /content/tmp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/tmp.zip\n",
            "   creating: content/tmp/\n",
            "  inflating: content/tmp/gcn.ckpt.meta  \n",
            "  inflating: content/tmp/gcn.ckpt.index  \n",
            "  inflating: content/tmp/checkpoint  \n",
            "  inflating: content/tmp/gcn.ckpt.data-00000-of-00001  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efd2T8hOgpq7"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from gcn.utils import *\n",
        "from gcn.inits import *\n",
        "from gcn.models import Model,MLP\n",
        "from gcn.layers import *\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk_YGF93WhCU"
      },
      "source": [
        "## Target graph initializzation\n",
        "\n",
        "To perform a prediction on a single graph, it is sufficient to process it with the same function introduced before. In this case, the target graph has been randomly generated thanks to the Networkx function.\n",
        "\n",
        "**To change the start (a) and the destination (b) nodes open this section and modify the $5^{th}$ cell.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "BkidIY8C-1f6"
      },
      "source": [
        "num_community = 1                                     \n",
        "node = [20 for i in range(num_community)]                \n",
        "GG = nx.random_partition_graph(node,.3,.0,seed=1996)       \n",
        "adj_GG = np.zeros((num_community, num_community))         \n",
        "\n",
        "    \n",
        "adj_sparse = sparse.csr_matrix(adj_GG)                    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X07MmGruQ2PH",
        "outputId": "768b7965-e2be-4eae-dc3d-4c22004acf9a"
      },
      "source": [
        "adj_sparse"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 0 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Q_MFwK6i-1f8"
      },
      "source": [
        "partition = GG.graph['partition']                       "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9cugwjtQkxP"
      },
      "source": [
        "H = GG.subgraph(partition[0])\n",
        "adj = list(nx.adjacency_matrix(H).todense().tolist())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXNtMHqwnfEq"
      },
      "source": [
        "a=7\n",
        "b=17\n",
        "adj = np.array(adj)\n",
        "adj[:,[a,0]] = adj[:,[0,a]]\n",
        "adj[[a,0],:] = adj[[0,a],:]\n",
        "adj[:,[b,19]] = adj[:,[b,19]]\n",
        "adj[[b,19],:] = adj[[19,b],:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6E4pJw9nfEr"
      },
      "source": [
        "adjlist = [[]]\n",
        "for element in adj:\n",
        "    adjlist[0].extend(element)                        \n",
        "    adjlist.append([])\n",
        "    \n",
        "adjlist = adjlist[0]\n",
        "adj_input = np.array(adjlist)\n",
        "features_sparse = sparse.csr_matrix(adj_input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt1RC0J_Qvk8",
        "outputId": "45067717-b8b7-4aee-e684-aee3a84802c7"
      },
      "source": [
        "features_sparse"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x400 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 92 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw1D2qDgRCTB",
        "outputId": "ae7f99fd-72f6-40fb-ee5d-b4a605caabc6"
      },
      "source": [
        "t = time.time()\n",
        "path = []                   \n",
        "try:\n",
        "    path.append([nx.shortest_path(GG.subgraph(partition[0]), source=a, target=b)])\n",
        "except:\n",
        "    path.append([0])\n",
        "t = time.time() - t\n",
        "t"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00020551681518554688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuoA2lJIRGoJ",
        "outputId": "179ee1df-a797-4fda-fd01-3648d8ee5c8f"
      },
      "source": [
        "path"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[7, 1, 4, 17]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MlqQXCKRNbM",
        "outputId": "bca093d5-3532-4fe3-f300-cb3c1e0318cc"
      },
      "source": [
        "y_pred = np.zeros((1, 19))\n",
        "y_pred"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gu2LwaYRnco",
        "outputId": "d96f9eaa-f7c9-4948-f306-5bd393119e94"
      },
      "source": [
        "pred_mask = np.zeros(1)\n",
        "pred_mask[0] = 1 \n",
        "pred_mask = np.array(pred_mask, dtype=np.bool)\n",
        "pred_mask"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srcmKtxdRsCr"
      },
      "source": [
        "adj = adj_sparse\n",
        "features = features_sparse   "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx503YBHWx7P"
      },
      "source": [
        "## GCN reinitialization\n",
        "\n",
        "**Do not execute this section if the training and the prediction are performed sequentially!**\n",
        "\n",
        "If you want to skip the training section you have to execute this section too because you need to specify the network in which the weights will be loaded to perform the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv51qfVuR-Wy"
      },
      "source": [
        "#Slightly modified version of Kipf 2017 Model\n",
        "\n",
        "class GCN(Model):\n",
        "    def __init__(self, placeholders, input_dim, **kwargs):\n",
        "        super(GCN, self).__init__(**kwargs)\n",
        "\n",
        "        self.inputs = placeholders['features']\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
        "\n",
        "        self.build()\n",
        "\n",
        "    def _loss(self):\n",
        "        for var in self.layers[0].vars.values():\n",
        "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
        "\n",
        "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
        "                                                  self.placeholders['labels_mask'])\n",
        "\n",
        "    def _accuracy(self):\n",
        "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
        "                                        self.placeholders['labels_mask'])\n",
        "\n",
        "    def _build(self):\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
        "                                            output_dim=FLAGS.hidden1,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,\n",
        "                                            dropout=True,\n",
        "                                            sparse_inputs=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
        "                                            output_dim=FLAGS.hidden2,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=tf.nn.relu,                      \n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "        \n",
        "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden2,\n",
        "                                            output_dim=self.output_dim,\n",
        "                                            placeholders=self.placeholders,\n",
        "                                            act=lambda x: x,\n",
        "                                            dropout=True,\n",
        "                                            logging=self.logging))\n",
        "\n",
        "    def predict(self):\n",
        "        return tf.nn.softmax(self.outputs)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ArH3n3DSR8E",
        "outputId": "91db4410-1f7e-45da-9c02-a4ce6006f9f5"
      },
      "source": [
        "def sparse_to_tuple(sparse_mx):\n",
        "    def to_tuple(mx):\n",
        "        if not sp.sparse.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "    \n",
        "\n",
        "def normalize_adj(adj): \n",
        "    adj = sp.sparse.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))                           \n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.sparse.diags(d_inv_sqrt, 0)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_adj(adj):\n",
        "    adj_normalized = normalize_adj(adj + sp.sparse.eye(adj.shape[0]))\n",
        "    return sparse_to_tuple(adj_normalized)\n",
        "\n",
        "\n",
        "\n",
        "num_supports = 1\n",
        "model_func = GCN\n",
        "support = [preprocess_adj(adj)]\n",
        "\n",
        "support"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(array([[0, 0]], dtype=int32), array([1.]), (1, 1))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah64mJ1ZSTRP",
        "outputId": "22e1c906-07e4-49de-f0b7-bb0306f32345"
      },
      "source": [
        "def preprocess_features(features):\n",
        "    rowsum = np.array(features.sum(1))                      \n",
        "    r_inv1 = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv2 = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv = np.multiply(r_inv1, r_inv2)\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.sparse.diags(r_inv, 0)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return sparse_to_tuple(features)\n",
        "\n",
        "\n",
        "features = preprocess_features(features)\n",
        "\n",
        "features[0].shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(92, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llu_EnxKTa0_"
      },
      "source": [
        "def masked_softmax_cross_entropy(preds, labels, mask):                      \n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_mean_square_error(preds,labels,mask):                            \n",
        "    loss = tf.nn.l2_loss(preds - labels)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "def masked_accuracy(preds, labels, mask):                                    \n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
        "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    mask /= tf.reduce_mean(mask)\n",
        "    accuracy_all *= mask\n",
        "    return tf.reduce_mean(accuracy_all)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SGHHQPuTlUJ",
        "outputId": "6318ecaf-26df-4bf1-b2b6-8c485da4eaa9"
      },
      "source": [
        "#Set random seed\n",
        "\n",
        "seed = 17\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "#Settings\n",
        "\n",
        "flags = tf.app.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "\n",
        "#Definition of network and training parameters\n",
        "\n",
        "flags.DEFINE_string('model', 'gcn', 'Model string.')  \n",
        "flags.DEFINE_float('learning_rate', 1e-4, 'Initial learning rate.')\n",
        "\n",
        "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train.')\n",
        "flags.DEFINE_integer('hidden1', 1024, 'Number of units in hidden layer 1.')\n",
        "flags.DEFINE_integer('hidden2', 1024, 'Number of units in hidden layer 2.')\n",
        "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
        "\n",
        "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
        "\n",
        "flags.DEFINE_integer('early_stopping', 300, 'Tolerance for early stopping (# of epochs).')\n",
        "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7f7594748910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAVjblBDTzYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99b1843f-ea69-449b-8a15-817994a14d20"
      },
      "source": [
        "#Extraction of the network configuration parameters\n",
        "\n",
        "remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
        "assert(remaining_args == [sys.argv[0]])\n",
        "if (FLAGS.model == 'gcn'):\n",
        "    support = [preprocess_adj(adj)]\n",
        "    num_supports = 1\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'gcn_cheby':\n",
        "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
        "    num_supports = 1 + FLAGS.max_degree\n",
        "    model_func = GCN\n",
        "elif FLAGS.model == 'dense':\n",
        "    support = [preprocess_adj(adj)] \n",
        "    num_supports = 1\n",
        "    model_func = MLP\n",
        "else:\n",
        "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
        "\n",
        "\n",
        "\n",
        "#Definition of placeholders as provided by Kipf\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "placeholders = {\n",
        "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
        "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
        "    'labels': tf.placeholder(tf.float32, shape=(None, y_pred.shape[1])),\n",
        "    'labels_mask': tf.placeholder(tf.int32),\n",
        "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
        "    'num_features_nonzero': tf.placeholder(tf.int32)  \n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#Declaration of the model\n",
        "\n",
        "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
        "\n",
        "\n",
        "\n",
        "#Session initialization\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "def evaluate(features, support, labels, mask, placeholders):\n",
        "    t_test = time.time()\n",
        "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
        "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
        "    return outs_val[0], outs_val[1], (time.time() - t_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2piRdHxUJwl"
      },
      "source": [
        "#Same as Kipf 2017, function to compile the training dictionary\n",
        "\n",
        "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
        "    feed_dict = dict()\n",
        "    feed_dict.update({placeholders['labels']: labels})\n",
        "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
        "    feed_dict.update({placeholders['features']: features})\n",
        "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
        "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
        "    return feed_dict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw-4JrRXW_9U"
      },
      "source": [
        "## Prediction pipeline\n",
        "\n",
        "In this pipeline the prediction is performed 10 times for each node to improve the performance of the model that achieves only 70% of accuracy for each node so, in a sequence of 5 node the probability to predict it right is exactly 16,8% and so most probably the output sequence will be wrong.\n",
        "\n",
        "In this case the sequence consists in  4 nodes and so 3 of them are predicted, the probability to obtain a right sequence is about 34% but it is slightly improved by an inference function that is explained in detail in the paper attached to this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYQPcsseXC8d"
      },
      "source": [
        "num_supports = 1\n",
        "model_func = GCN\n",
        "features = preprocess_features(features_sparse)\n",
        "support = [preprocess_adj(adj)]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_juE0fYcpWj",
        "outputId": "95b8c694-9f6d-40ab-9d72-099c6c2e06e0"
      },
      "source": [
        "X=7\n",
        "predicted_path = [X]\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "model.load(sess)                                                                              #Loading weights from training pipeline\n",
        "\n",
        "while (X!=b):\n",
        "\n",
        "  #t = time.time()\n",
        "  \n",
        "  # Construct feed dictionary\n",
        "  feed_dict = construct_feed_dict(features, support, y_pred, pred_mask, placeholders)\n",
        "  feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
        "\n",
        "  results = []\n",
        "  for i in range(10):                                                                           #Appending a new row containing the output of the inference to results[] for each iteration\n",
        "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.predict()], feed_dict=feed_dict)\n",
        "    test_cost, test_acc, test_duration = evaluate(features, support, y_pred, pred_mask, placeholders)\n",
        "    results.append(np.argmax(outs[3][0]))\n",
        "\n",
        "  count = np.bincount(results)\n",
        "\n",
        "  if (len(count) == 19):                                                                        #If the system predicts node 19 less than 7 times ignore its occurrences in the results[] vector\n",
        "    if (count[18] < 7):\n",
        "      count[18] = 0\n",
        "  \n",
        "  result = np.argmax(count)                                                                     #Keep as output the node with more occurrences\n",
        "  \n",
        "  #print(results)\n",
        "  #print(\"Time elapsed: \", time.time()-t)\n",
        "\n",
        "  X = (result+1)\n",
        "\n",
        "  if (X==a):                                                                                    #The node with the sorce index has been sobstituted with node number 0 at the beginning\n",
        "    X = 0\n",
        "  elif X==b:                                                                                    #The node with the destination index has been sobstituted with node number 19 at the beginning\n",
        "    X = 19\n",
        "  elif X==19:                                                                                   #The node with the index 19 has been sobstituted with the destination node at the beginning\n",
        "    X = b\n",
        "  predicted_path.append(X)\n",
        "\n",
        "  adj = np.array(nx.adjacency_matrix(H).todense().tolist())                                     #Now the predicted node become the node number 0 because it is considered as the new source node\n",
        "  adj[:,[X,0]] = adj[:,[0,X]]\n",
        "  adj[[X,0],:] = adj[[0,X],:]\n",
        "\n",
        "  adjlist = [[]]\n",
        "  adj = list(adj)\n",
        "  for element in adj:\n",
        "      adjlist[0].extend(element) \n",
        "      adjlist.append([])\n",
        "      \n",
        "  adjlist = adjlist[0]\n",
        "  adj_input = np.array(adjlist)\n",
        "  features_sparse = sparse.csr_matrix(adj_input)\n",
        "\n",
        "  features = features_sparse   \n",
        "\n",
        "  features = preprocess_features(features)\n",
        "\n",
        "#print(predicted_path)\n",
        "processed_pred_path = sorted(set(predicted_path), key=predicted_path.index)\n",
        "print(\"\\n\")\n",
        "if (processed_pred_path == path[0][0]):\n",
        "  print(\"The predicted path matches the Dijkstra shortest path!\")\n",
        "elif (len(processed_pred_path) == len(path[0][0])):\n",
        "  print(\"The predicted path DOESN'T match the Dijkstra shortest path but they are the same length!\")\n",
        "elif (len(processed_pred_path) != len(path[0][0])):\n",
        "  print(\"The predicted path DOESN'T match the Dijkstra shortest path and they are NOT the same length!\")\n",
        "\n",
        "print(\"The predicted path is: \", processed_pred_path)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from tmp/gcn.ckpt\n",
            "Model restored from file: tmp/gcn.ckpt\n",
            "\n",
            "\n",
            "The predicted path matches the Dijkstra shortest path!\n",
            "The predicted path is:  [7, 1, 4, 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "VFspX0azQXWr",
        "outputId": "2327e8b8-8ccd-4027-da8b-9ca568c41933"
      },
      "source": [
        "nx.draw(GG, with_labels = True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}